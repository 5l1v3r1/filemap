#!/usr/bin/python 
#
# Public Domain License:
#
# This program was prepared by Los Alamos National Security, LLC at
# Los Alamos National Laboratory (LANL) under contract
# No. DE-AC52-06NA25396 with the U.S. Department of Energy (DOE). All
# rights in the program are reserved by the DOE and Los Alamos
# National Security, LLC.  Permission is granted to the public to copy
# and use this software without charge, provided that this Notice and
# any statement of authorship are reproduced on all copies.  Neither
# the U.S. Government nor LANS makes any warranty, express or implied,
# or assumes any liability or responsibility for the use of this
# software.
#
# Author: Mike Fisk <mfisk@lanl.gov>
#

import ConfigParser, sys, optparse, random, urlparse, subprocess, os, glob, errno, string, socket, time, traceback, sha, pydoc, tempfile, re, socket, cStringIO, base64, stat, cPickle, pwd, grp, shlex

EchoCommands = False

nonalphanumre = re.compile('\W')

def escape(s):
   (s, num) = nonalphanumre.subn(lambda m: "=%02X" % ord(m.group()), s)
   return s

def rm_rf(path):
   for (dirpath,dirs,files) in os.walk(path, topdown=False):
      for f in files: 
         os.unlink(dirpath + "/" + f)

      os.rmdir(dirpath)

def mkdirexist(path):
   try:
      os.makedirs(path)
   except OSError, e:
      if e.errno == errno.EEXIST:
         pass
      else:
         raise e

class FmLocations:
   def __init__(self, config=None, locs=[], stdin=False):
      self.locs = locs # Allow pick() to derive an instance with a subset of the classes

      if config:
         self.config = config

      else:
         self.config = ConfigParser.SafeConfigParser()

         if stdin:
            # Read config file from stdin
            self.config.readfp(sys.stdin)

            # Keep a copy of the config around so our children can reference it
            fd, name = tempfile.mkstemp()
            os.environ['FMCONFIG'] = name
            fh = os.fdopen(fd, 'w')
            self.config.write(fh)
            fh.close()

         else:
            filename = os.environ.get('FMCONFIG')
            #print >>sys.stderr, "Env filename is", filename
            if not filename:
               if os.path.isfile("filemap.conf"):
                  filename = "filemap.conf"
               else:
                  filename = "/etc/filemap.conf"

            # Read config file from stdin
            r = self.config.read(filename)
            if not r:
               raise Exception("Unable to locate config file.  Set FMCONFIG or place filemap.conf in . or /etc")

            #print >>sys.stderr, "Reading config", filename, self.config.sections()
            os.environ['FMCONFIG'] = filename

         self.replication = int(self.config_get("global", "replication", "1"))

      # Unless we were passed an explicit list of locations, grab all
      # from the config file.
      if not locs:
         for s in self.config.sections():
            if s == "global": continue
            l = FmLocation()
            l.name = s
            l.rootdir = os.path.expanduser(self.config_get(s, "rootdir"))
            l.jobdir = l.rootdir + "/jobs"
            l.hostname = self.config_get(s, "hostname")
            l.syncdir = os.path.expanduser(self.config_get(s, "syncdir"))

            l.fmcmd = os.path.expanduser(self.config_get(s, "fm", "fm"))
            l.sshcmdstr = os.path.expanduser(self.config_get(s, "ssh", "ssh -o 'ControlMaster auto' -KAS ~/.ssh/%l-%r@%h:%p", raw=True))
            l.sshcmd = shlex.split(l.sshcmdstr, comments=True)
            l.rsynccmd = shlex.split(self.config_get(s, "rsync", "rsync", raw=True), comments=True)
            l.rsynccmd[0] = os.path.expanduser(l.rsynccmd[0])

            self.locs.append(l)

      # Nail open SSH connections for low-latency muxing
      try:
         for l in self.locs:
            if l.hostname:
               #print "Setting-up persistent SSH connection to", l.hostname
               #subprocess.check_call(l.sshcmd + ["-o", "ControlMaster auto", "-fN", l.hostname], stdin=None)
               pass
      except:
         traceback.print_exc()
         print >>sys.stderr, """Warning, unable to use "ssh -o 'ControlMaster auto' -fN"; Will do without."""

   def config_get(self, section, key, defval=None, raw=False):
      if self.config.has_option(section, key):
         return self.config.get(section, key, raw=raw)
      elif self.config.has_option("global", key):
         return self.config.get("global", key, raw=raw)
      else:
         return defval

   def thisis(self, n):
      # This sets of an easy to use member to get the info for this Location
      self.this = self.locs[n]
      self.thisnum = n

   def pickn(self, seed=None):
      """Pick some pseudo-random locations.  The number of locations
      chosen is based on the replication factor in the config.
      Specify a seed to get deterministic results (e.g. for a given
      extension number.)  The return value is the list of indices into
      the current locations."""

      #print "pick seed=", seed
      candidates = range(0, len(self.locs))
      used = []
      if seed: random.seed(seed)

      for i in range(0, self.replication):
         if not candidates:
            print >>sys.stderr, "Cannot pick %d locations from %d available" % (n, len(self.locs))
            break
         l = random.choice(candidates)
         used.append(l)
         candidates.remove(l)
     
      #print >>sys.stderr, "pick", seed, "->", used

      return used
         
   def pick(self, seed=None, selfIsNoop=False):
      """Return a new FmLocations instance that uses a subset of the
locations of this instance."""

      used = self.pickn(seed)
      if selfIsNoop and self.thisnum in used: 
         used.remove(self.thisnum)
      return FmLocations(self.config, [self.locs[i] for i in used])
         
   def forAllLocal(self, cmd, stdout=None):
      """Same ase forAll(), but CMD will be run as arguments to "fm _local"."""

      cmd = ["%(FM)", "_local", "-n", "%(NODENUM)"] + cmd
      fp = cStringIO.StringIO()
      self.config.write(fp)

      return self.forAll(cmd, subst=True, stdout=stdout, stdinstr=fp.getvalue())

   def forAll(self, Cmdv, src=[], dst=[], absolutes=[], subst=False, trailer=None, glob=True, stdinstr=None, stdout=None, stderr=None, call=False, once=False):
      if src and type(src) != type([]): src = [src]
      if dst and type(dst) != type([]): dst = [dst]

      if subst:
         def sub(loc, cmdword, num):
            return cmdword.replace("%(ROOTDIR)", loc.rootdir).replace("%(SYNCDIR)", loc.syncdir).replace("%(NODENUM)", str(num)).replace("%(FM)", loc.fmcmd)
            
      if stdinstr:
         std=subprocess.PIPE

      procs = Procs(stdinstr=stdinstr, stdout=stdout, stderr=stderr)

      i = 0
      for loc in self.locs:
         if subst: 
            cmd = [sub(loc,n,i) for n in Cmdv]
            a = [sub(loc,s,i) for s in absolutes]
         else:
            cmd = list(Cmdv)
            a = absolutes

         s = [loc.rootdir + "/" + f for f in src]
         d = [loc.rootdir + "/" + f for f in dst]

         if loc.hostname:
            cmd = loc.sshcmd + [loc.hostname] + cmd + s + d
         else:
            if s and glob:
               s = multiglob(s)
               if not s: continue

            if d and glob:
               d = multiglob(d)
               if not d: continue

            if a and glob:
               a = multiglob(a)
               if not a: continue

            cmd += s + d + a
            
         if trailer:
            cmd += trailer

         procs.Popen(cmd, call=call)

         i += 1

         # Stop now if "once" argument was set
         if once and i: return procs

      return procs
   
   def put(self, src, dst, procs=None):
      if not procs: procs = Procs()

      s = multiglob([src])

      if not s:
         print >>sys.stderr, "No such file:", src
         return procs

      for loc in self.locs:
         d = loc.rootdir + "/" + dst

         if loc.hostname:
            d = loc.hostname + ":" + d

         cmd = loc.rsynccmd + ["-a", "-e", loc.sshcmdstr] + s + [d]

         procs.Popen(cmd)

      return procs

   def get(self, args, procs=None, outfile=sys.stdout, relative=True):
      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.add_option("-c", "--cat", action="store_true", help="Cat files to stdout")
      p.add_option("-n", "--name", action="store_true", help="Show file name when catting files")
      (options, args) = p.parse_args(args)

      rsyncflags = "-rptgo"
      if relative: rsyncflags += "R"

      if options.cat:
         assert(len(args) > 0)
         dst = tempfile.mkdtemp() + "/"
         src = args
      else:
         assert(len(args) > 1)
         dst = args[-1]
         src = args[:-1]

      if not procs: procs = Procs()

      for loc in self.locs:
	 # To make relative paths shorter in rsync >= 2.6.7 , put "/./" 
         # in the path between the rootdir and the relative path
         s = [loc.rootdir + "/./" + f for f in src]
         #print loc.rootdir, src, s

         if loc.hostname:
            s = [loc.hostname + ":" + string.join(s)]
         else:
            s = multiglob(s)
            if not s:
               #print >>sys.stderr, "No such file:", src
               continue

         # Cleanup the path to encourage rsync to get "/./" magic right
         s = [i.replace('//','/') for i in s]

         cmd = loc.rsynccmd + [rsyncflags, "--copy-unsafe-links", "-e", loc.sshcmdstr] + s + [dst]
         print >>sys.stderr, cmd

         procs.Popen(cmd)

      procs.collect(ignoreErrors=True)

      if options.cat:
         empty = True
         for (dirpath, dirnames, filenames) in os.walk(dst):
            for f in filenames:
               fname = os.path.join(dirpath, f)
               if os.path.getsize(fname):
                  if options.name:
                     relname = fname[len(dst):]
                     print >>outfile, "=== %s ===" % relname
                     empty = False
                  outfile.writelines(file(fname))
         if options.name and not empty:
            print >>outfile, "======"

         rm_rf(dst)

      return procs
         
class FmLocation:
   def __init__(self):
      pass

            


def parseLocationURL(locurl):
   # We define rsync+ssh as a scheme
   urlparse.uses_netloc.append("file")
   urlparse.uses_netloc.append("ssh")

   u = urlparse.urlsplit(locurl)

   # Fix-up the results
   if not u.scheme and not u.netloc and u.path[0] == '/':
      u = urlparse.urlparse("file://" + u.path)

   return u


class ErrorFile:
   """This class implements a few file-like methods so that it can be used
   in place of a real file in some cases.  Whatever is written to this file
   is printed on stderr with a static prefix (like syslog)."""

   def __init__(self, prefix):
      self.prefix = prefix

   def write(self, buf):
      print >> sys.stderr, prefix, ":", buf

   def fileno(self): return sys.stderr.fileno()

class Procs:
   """This class is used to launch some number of child processes and reap them.
   It provides methods to instantiate a process for each node."""

   procs = []

   def __init__(self, stdinstr=None, stdout=None, stderr=None, stdin=None):
      self.stdout = stdout
      self.stderr = stderr
      self.stdin = stdin
      self.stdinstr = stdinstr

      if stdinstr:
         self.stdin = subprocess.PIPE

   def Popen(self, lst, prefix=None, call=False):
      if call:
         if EchoCommands: print >>sys.stderr, lst
         subprocess.call(lst)
         return 

      if prefix == None: prefix = len(self.procs)
      if self.stderr:
         err = self.stderr
      else:
         err = ErrorFile(prefix)

      if EchoCommands: print >>sys.stderr, lst
      r = subprocess.Popen(lst, stdin=self.stdin, stdout=self.stdout, stderr=err)
      self.procs.append(r)

      if self.stdinstr:
         r.stdin.write(self.stdinstr)
         r.stdin.close()

   def collect(self, ignoreErrors=False):
      errors = 0
      total = len(self.procs)
      for p in self.procs:
         if p.wait(): errors += 1

      if not ignoreErrors and errors:
         print >>sys.stderr, "Error, %d/%d subprocess(es) returned error" % (errors, total)
         sys.exit(-1)

      return (errors == 0)

# The design is as follows:
#   1. Jobs are submitted in an uncoordinated manner, so they have unique
#      identifiers that aren't generated by the user. but that are identical
#      across all nodes running a job. 
#   2. To manage the problem of listing jobs and removing jobs, we use a 
#      directory structure to contain all current jobs.  Deleting a file should
#      (eventually) lead to a job not running further.
#   3. New jobs should be invoked synchrously by calling a job scheduler with a
#      hint that points to the job.  Failure to send this hint should only delay
#      the job scheduler discovering the job since it should periodically poll
#      for changes in the job directory.
#   4. We assume out of band replication of the job dir across nodes (via NFS, 
#      rsync, etc.)

class JobScheduler:
   """JobScheduler maintains a set of child processes working on a set of jobs."""

   def __init__(self, options, numthreads=None):
      self.jobs = {}  # A dictionary of job objects indexed by job name
      self.procs = {} # A dictionary of subprocess.Popen objects index by pid
      self.options = options

      self.freethreads = numthreads 
      if not self.freethreads:
         self.freethreads = 4 #XXX: Use configfile setting

   def ReadJobDir(self):
      """Check for new/removed jobs"""

      jobs = os.listdir(self.options.jobdir)
      removed = set(self.jobs) - set(jobs)
      recent = set(jobs) - set(self.jobs)
      for j in removed:
         print "Job", j, "removed"
         del self.jobs[j]

      for j in recent:
         print "Job", j, "added"
         try:
            self.jobs[j] = FmJob(self.options.jobdir + "/" + j)
         except:
            traceback.print_exc()
            print >>sys.stderr, "Error parsing job description", j, "; skipping"

   def RunUntilDone(self):
      """Run until all jobs complete."""

      # Run as long as there are jobs (new data may arrive later)
      while True:
         while self.RunUntilNoInput():
            break

         if not len(self.jobs): 
            return

         # If we get here, then we ran out of stuff to do, so wait a bit before trying again
         #print >>sys.stderr, "Nothing to do, waiting for work"

         time.sleep(1)

   def RunOnce(self):
         """Try to launch a work item and return True on success (None->nothing to do)"""

         self.freethreads -= 1

         # Look for something with work to do
         for job in self.jobs.keys(): #XXX, Should randomize order
            proc = self.jobs[job].compute(self.options)
            if proc: 
               #print >>sys.stderr, "Launched", proc.pid
               self.procs[proc.pid] = proc
               return True
            else:
               # No more work to do, check to see if job is complete
               # Note, job cannot be complete if parent is still active
               j = self.jobs[job]
               if not j.continuous and not j.running and (not j.parent or not self.jobs.has_key(j.parent)):
                  self.JobDone(job)

         self.freethreads += 1
         return None

   def JobDone(self, job):
      """A job has run to completion, remove it locally."""

      del self.jobs[job]
      #print >>sys.stderr, "Job", job, "done"
      os.unlink(self.options.jobdir + "/" + job)

   def RunUntilNoInput(self):
      """Run until there is no more input on it.  Return True iff we did something"""

      didSomething = False
      sleep = 0.0001

      while True:
         if self.freethreads:
            self.ReadJobDir()  
            if self.RunOnce():
               didSomething = True
            else:
               # Nothing to do right now

               if not len(self.procs): 
                  # Nothing still running, so return
                  #print >>sys.stderr, "RunOnce() not true; no running procs"
                  return didSomething 

               # Don't return because we're still working,
               # but wait before we look for more work to do.
               #print >>sys.stderr, "Wait for completion (and/or more work)"

               # Sleeping hurts latency for detecting new inputs, but avoids busy waits.
               # So we do an exponential backoff in how long we sleep with a max of 1 second
               time.sleep(sleep)
               sleep *= 10
               if sleep > 1: sleep = 1  # Max out at 1 sec sleep

               self.CollectChildren() # Won't block
         else:
            self.CollectChildren() # Will block

   def CollectChildren(self): 
      # This is a loop in case multiple children are exited
      while len(self.procs):
         if not self.freethreads:
            # Maxed out on threads, so wait for a child to finish
            #print >>sys.stderr, "Wait for a child to exit"
            opts = 0
         else:
            # Check for children, but don't block waiting
            opts = os.WNOHANG

         (pid, status) = os.waitpid(-1, opts) 
         if pid:
            #print "Process", pid, "exited with status", status
            self.finalize(self.procs[pid], status)
            del self.procs[pid]
            self.freethreads += 1
         else:
            # No children pending, so return
            return

   def finalize(self, p, status):
      #print >>sys.stderr, "Job completed", status
      print >>p.statfile, status
      p.statfile.close()
      p.statfile = None
      
      os.rename(p.outfilename + ".new", p.outfilename)
      p.job.running -= 1 

      del p
     
class FmJob:
   """Each FmJob object represents a job, as specified in a job file, and
    provides methods for identifying and processing inputs for that job."""

   def __init__(self, fname):
      config = ConfigParser.SafeConfigParser()

      processed = config.read(fname)
      if not processed:  
         raise IOError(errno.ENOENT, "Could not open file", fname)

      self.jobname = os.path.basename(fname)
      self.cmd = config.get("mrjob", "cmd", raw=True)
      self.inputs = config.get("mrjob", "inputs")
      self.continuous = config.has_option("mrjob", "continuous")
      self.reduce = config.has_option("mrjob", "reduce")
      if config.has_option("mrjob", "parent"):
         self.parent = config.get("mrjob", "parent")
      else:
         self.parent = None

      # Number of threads currently processing data for this job.
      self.running = 0

   def compute(self, options):
      inputs = multiglob([options.rootdir + "/" + x for x in self.inputs.split()])

      if self.reduce:
         exts = [os.path.splitext(i)[1] for i in inputs]
         exts = list(set(exts))  # Get unique extensions

         for ext in exts:
            # Don't process .d directories as named inputs (feedback loop)
            if ext == ".d": continue

            # Apply all of the files with the same extension,
            # but only if that extension belongs on this node.
            choices = Locations.pickn(ext)
            if Locations.thisnum not in choices:
               #print >>sys.stderr, Locations.thisnum, "hashes", ext, "to", choices
               continue

            #Get list of files with this extension
            xlen = len(ext)
            files = []
            for i in inputs:
               if i[-xlen:] == ext:
                  files.append(i)

            outfilename = "/reduce/" + self.jobname + ext

            p = self.computeItem(files, outfilename, options)
            if p: 
               #print >>sys.stderr, "Reduce ext", ext, "on node", Locations.thisnum, files, "of", self.inputs

               return p

         return None # Nothing to do

      for i in inputs:
         if i[-2:] == ".d": 
            # Don't process .d directories as named inputs (feedback loop)
            continue

         bname = os.path.basename(i)

         relativename = i[len(options.rootdir):]
         outfilename = relativename + ".d/" + escape(self.cmd)
         
         p = self.computeItem(i, outfilename, options)
         if p: return p

      return None # Nothing to do

   def computeItem(self, inputs, outfilename, options):
      if type(inputs) != type([]):
         inputs = [inputs]

      statfilename = options.syncdir + "/" + outfilename
      mkdirexist(os.path.dirname(statfilename))
      try:
         statfile = os.fdopen(os.open(statfilename, os.O_CREAT|os.O_EXCL|os.O_WRONLY), "w")
      except OSError, e:
         if e.errno == errno.EEXIST:
            # This is a common case; somebody has already grabbed
            # the synchronization file, so we don't need to process
            # this file.
            #print >>sys.stderr, "Did not win", statfilename
            return None
         else:
            traceback.print_exc()
               #XXX: re-raise
            raise "Error"

      # Now we hold the "lock" the statfile, so proceed...

      print >>statfile, socket.gethostname(), os.getpid()
      statfile.flush()

      obase = options.rootdir + "/" + outfilename
      oname = obase + ".0"
      ename = obase + ".stderr"
      mkdirexist(os.path.dirname(oname))
      sout = os.fdopen(os.open(oname + ".new", os.O_CREAT|os.O_WRONLY), "w")
      serr = os.fdopen(os.open(ename, os.O_CREAT|os.O_WRONLY), "w")

      cmd = self.cmd.replace('%(FM)', options.fmcmd)

      if '%(input)' in cmd:
         cmd = cmd.split()
         iidx = cmd.index('%(input)')
         cmd[iidx:iidx] = inputs
      else:
         cmd = cmd.split() + inputs

      os.environ['FMOUTPUT'] = obase
      try:
         p = subprocess.Popen(cmd, stdout=sout, stderr=serr)
      except:
         print >>sys.stderr, "Error", sys.exc_value, "executing", cmd
         return None

      p.statfile = statfile
      p.errfilename = ename
      p.outfilename = oname
      p.job = self
      self.running += 1
      sout.close()
      serr.close()

      print >>sys.stderr, "computeItem", cmd, ">", oname, socket.gethostname(), p.pid
      return p

class CommandSetBase(object):
   """This is a base-class for defining methods which are used as
   command line-based commands.  You should inherit from it and define
   methods.  All methods will be callable.  Usage and help information
   will only include methods that do not begin with an underscore."""

   def __method(self, mname):
      return self.__class__.__dict__[mname]

   def __usage(self):
      """Return a multi-line usage string based on the defined methods and
their doc strings."""

      usage = ''
      for cmd in self.__class__.__dict__:
         if cmd[0] == "_": continue
         method = self.__method(cmd)
         usage += pydoc.text.indent(pydoc.text.document(method)).replace("(self, args)","")
      return usage

   def __init__(self, args):
      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.set_usage("""%prog command args...\nSupported commands:\n""" + self.__usage())
      (options, args) = p.parse_args(args)
   
      if not args: 
         p.print_help()
         sys.exit(1)

      try:
         method = self.__method(args[0])

      except:
         print >>sys.stderr, "Unknown command ", args[0]
         p.print_help()
         sys.exit(1)

      method(self, args[1:])

class FmCommands(CommandSetBase):
   _locs = None

   def not__init__(self, args):
      self._locs = None
      CommandSetBase.__init__(self, args)

   def _Locations(self):
      if not self._locs: 
         self._locs = FmLocations()
      return self._locs

   def split(self, args):
      """
-n # [-r regex] infile outfile 

aSplit an inputfile into n pieces.  By default, the first
whitespace-delimited field is used as the key.  All lines with the
same key will be placed in the same output file.  The -r option can be
used to specify a Perl-compatible regular expression that matches the
key.  If the regex contains a group, then what matches the group is
the key; otherwise, the whole match is the key.

The root of the output file names must be specified either on the
command line or in the FMOUTPUT environment variable (which is set for
all programs running as FM jobs).  
"""
      p = optparse.OptionParser()
      p.add_option("-n", "--nways", help="Number of output files to use")
      p.add_option("-r", "--regex", action="store", help="Regex that matches the key portion of input lines")
      (options, args) = p.parse_args(args)
      options.nways = int(options.nways)

      ofile = os.environ.get('FMOUTPUT')
      if not ofile:
         if len(args) < 2:
            print >>sys.stderr, "Output filename must be specified in FMOUTPUT environment variable or command-line argument."
            return False
         ofile = args[-1]

      assert(len(args))
      infile = args[0]

      #print >>sys.stderr, "Writing to", ofile

      if not options.nways:
         print >>sys.stderr, "-n option required"
         return False
      
      if not options.regex:
         options.regex = '^([^\s]*)'
         
      options.regex = re.compile(options.regex)

      files = []
      for i in range(0, options.nways):
         fname = ofile + "." + str(i+1)
         files.append(file(fname, "w"))

      for line in file(infile):
         key = options.regex.search(line)
         if key:
            g = key.groups()
            if len(g):
               key = g[0]
            else:
               key = key.group(0)
         else:
            print >>sys.stderr, "Key not found in line:", line.rstrip()

         i = hash(key) % options.nways
         files[i].write(line)

      for f in files: f.close()

   def kill(self, args):
      """
job-id...
Kill the specified job(s)
"""
      if len(args) < 1:
         print >>sys.stderr, "Must specify a JobId to kill"
         return False
      args = ["/jobs/" + a for a in args]
      self._Locations().forAll(["rm", "-f"], args).collect()

   def mv(self, args):
      """src... dst\nRename a file within the virtual store."""
      if len(args) < 2:
         print >>sys.stderr, "mv requires at least 2 arguments"
         return False

      dst = args[-1]
      each = args[:-1]
      if len(each) > 1: dst += "/"
 
      return self._Locations().forAll(["mv"], each, dst).collect()

   def mkdir(self, args):
      """Make a directory (or directories) on all nodes.  Has unix "mkdir -p" semantics."""
      # Make sure destination exists
      self._Locations().forAll(["mkdir", "-p"], args, glob=False).collect()

   def jobs(self, args):
      """Show all of the jobs still active."""

      tmpdir = tempfile.mkdtemp()

      self._Locations().get(["/jobs/*", tmpdir], relative=False)
      fmt = "%-28s %-15s %s"
      print fmt % ("Job ID","Command", "Inputs")
        
      for f in os.listdir(tmpdir):
         j = FmJob(os.path.join(tmpdir, f))
         print fmt % (f, j.cmd, j.inputs)

      rm_rf(tmpdir)
  
   def df(self, args):
     """Show free space on each node."""
     self._Locations().forAll(["df", "-h"], "/", glob=False).collect()

   def chmod(self, args):
     """Change permissions on all nodes."""
     self._Locations().forAll(["chmod", args[0]], args[1:]).collect()

   def store(self, args):
      """src... dst

Copy the specified file(s) into the virtual store.  
"""
      assert(len(args) > 1)
      dst = args[-1]
      args = args[:-1]
      if len(args) > 1: dst += "/"

      procs = Procs()
      # Iterate over args in batches.
      # Since hashing may not be even and we want each node to be doing something,
      # we use 1.5 * NODES / replication as the batch size
      while args:
         for i in range(0, int(1.5 * len(self._Locations().locs) / self._Locations().replication)):
            a = args.pop(0)
            if not a: break
            self._Locations().pick().put(a, dst+"/", procs=procs)
         procs.collect()
         

   def map(self, args):
      """
[-f] [-c] [-i inputglob] cmd [args...]
[-f] [-c] [-i inputglob] cmd [args...] | cmd [args...] |...

Run the specified command on each input file (in the virtual store)
described by the inputglob.  Multiple inputglob arguments can be
given.  The -c option says that the commond should continue to run on
new inputs as they arrive.

Multiple commands can be chained together with |.  Each output file of
the first command becomes an input for the next command in the
pipeline.

The -f option says that any previously cached output should be ignored
and the program re-run.
"""

      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.add_option("-i", "--inputglob", action="append", help="Glob of input files (in root)")
      p.add_option("-c", "--continuous", action="store_true", help="Continue to look for new input files to arrive")
      p.add_option("-f", "--fresh", action="store_true", help="Do not use any cached output")
      (options, cmd) = p.parse_args(args)
      
      cmds = string.join(cmd).split("|")
      iglobs = options.inputglob
      assert (type(iglobs) == type([]))

      jobname = None
      for cmd in cmds:
         reduce = False
         cmd = cmd.strip()
         if cmd[0] == ">":
            cmd = cmd.lstrip(">")

            # First, re-distribute the data
            jobname = self._MapComponent("%(FM) _local restore -s", iglobs, options, parent=jobname)

            # Barrier; Wait for that job (and its parents) to finish:
            # But first, make sure a scheduler is running
            self._Locations().forAllLocal(["sched"]).collect()

            print >>sys.stderr, "Waiting for distribute to complete before reducing.  Job", jobname
            self._Locations().forAllLocal(["wait", jobname]).collect()

            # There is no change to the iglobs directory since restore puts files in the same virtual path they started in
            reduce = True

         jobname = self._MapComponent(cmd, iglobs, options, reduce=reduce, parent=jobname)

         # Each component of pipeline uses previous output as input
         if reduce:
	    errglobs += ["/reduce/" + jobname + ".stderr"]
            iglobs = ["/reduce/" + jobname + ".*[0-9]"]
         else:
            errglobs = [i + ".d/" + escape(cmd) + ".stderr" for i in iglobs]
            iglobs = [i + ".d/" + escape(cmd) + ".*[0-9]" for i in iglobs]

      self._Locations().forAllLocal(["sched"]).collect()

      print >>sys.stderr, "Waiting for completion.  Job", jobname
      self._Locations().forAllLocal(["wait", jobname]).collect()
      print >>sys.stderr, "Looking for errors:", errglobs
      self.get(["-cn"] + errglobs, outfile=sys.stderr)
      self.get(["-c"] + iglobs)

   def get(self, *args, **kwargs):
      return self._Locations().get(*args, **kwargs)

   def cat(self, args):
      procs = self._Locations().forAll(["cat"], args, stdout=subprocess.PIPE)
      for f in procs.procs:
            sys.stdout.writelines(f.stdout)
      return procs.collect(ignoreErrors=True)

   def _local(self, args):
      """This is for slave (remote) instantiations."""
      FmLocalCommands(args)

   def ls(self, args):
      """Simple file & directory listing."""
      if not args: args = ["/"]
      procs = self._Locations().forAllLocal(["ls"] + args, stdout=subprocess.PIPE)
      ls = {}
      for p in procs.procs:
         pkl = p.stdout.read()
         if not pkl: continue

         try:
            files = cPickle.loads(pkl)
         except:
            print >>sys.stderr, "Error", sys.exc_value, "parsing remote ls results:", pkl
            return -1

         for f in files:
            fname = f['name']
            if not ls.has_key(fname): ls[fname] = []
            ls[fname] += [f]

      keys = ls.keys()
      keys.sort()
      for k in keys:
         base = ls[k][0]
         nodes = [base['node']]
         isdir = base['perms'][0] == 'd'
         if isdir: base['size'] = 0
         criteria = ['perms', 'user', 'group', 'size']
         if not isdir: criteria.append('size')
         

         for i in ls[k][1:]:
            if isdir: i['size'] = 0

            # See if this node's instance matches the right criteria to be
            # collapsed into a single output line
            match = True
            for a in criteria:
               if base[a] != i[a]: 
                  # Does not match
                  match = False
                  break

            if match:
               # If you specify the same file twice, we don't want
               # the node number(s) duplicated in the node list
               if i['node'] not in nodes:
                  nodes.append(i['node'])
            else:
               # No match, so output a line for this instance
               print self._lsline(i)

         # Update the nodelist for this base instance and print it out
         base['node'] = nodes
         print self._lsline(base)
         
      return 

   def _lsline(self, attrs):
      size = "%-.2g" % attrs['size']
      size = size.replace("e+", "e").replace("e0", "e")

      nodes = attrs['node']
      if type(nodes) != type([]): nodes = [nodes]

      if len(nodes) > 1:
         attrs['mtime'] = '-'
      else:
         lt = time.localtime(attrs['mtime'])
         if lt[0] == time.localtime()[0]:
            attrs['mtime'] = time.strftime("%b %d %H:%M", lt)
         else:
            attrs['mtime'] = time.strftime("%b %d  %Y", lt)
               
      if len(nodes) == len(self._Locations().locs):
         nodes = '*'
      else:
         nodes = [str(i) for i in nodes]
         nodes = string.join(nodes,',')

      return "%s %3s %8s %8s %6s %12s %s" % (attrs['perms'], nodes, attrs['user'], attrs['group'], size, attrs['mtime'], attrs['name'])

   def rm(self, args):
      """
[flags] file...

Remove empty file or directory.  Normal rm flags are passed through.
"""
      options = []
      for a in args:
         if a[0] == "-":
            options.append(a)
         else:
            args = args[len(options):]

            if not args: args = "/"

            break

      return self._Locations().forAll(["rm", "-f"] + options, args).collect()

   def rmdir(self, args):
      """
[flags] dir...

Remove empty directory (or directories).  Normal rmdir flags are passed through.
"""
      options = []
      for a in args:
         if a[0] == "-":
            options.append(a)
         else:
            args = args[len(options):]

            if not args: args = "/"

      return self._Locations().forAll(["rm", "-f"] + options, args).collect()

   def init(self, args):
      self._Locations().forAll(["mkdir", "-p", "%(SYNCDIR)", "%(ROOTDIR)"], ["/jobs", "/reduce"], subst=True, glob=False).collect()
 
   def _MapComponent(self, cmd, iglobs, options, reduce=False, parent=None):
      jobdesc = "[mrjob]\ncmd = %s\ninputs = %s\n" % (cmd, string.join(iglobs))
      if parent: jobdesc += "parent = %s\n" % (parent)

      if options.continuous:
         jobdesc += "continuous = True\n"

      if reduce:
         jobdesc += "reduce = True\n"

      hash = sha.sha(jobdesc).digest()
      hash = base64.b64encode(hash, '+_')  # Normal b64 but / is _
      hash = hash.rstrip('\n\r =')
      jobfilename = "/tmp/" + hash
      jobfile = open(jobfilename, "w")
      jobfile.write(jobdesc)
      jobfile.close()

      if options.fresh:
         if reduce:
            statfiles = ["%(SYNCDIR)/reduce/" + hash + ".*[0-9]"]
         else:
            statfiles = ["%(SYNCDIR)/" + g + ".d/" + escape(cmd) for g in iglobs] 

         # Delete sync files so that computation is forced to (re-)execute
         # These files are shared, so our forAll() is set to do only one node (once=True)
         self._Locations().forAll(["rm", "-f"], absolutes=statfiles, once=True, subst=True).collect()

      # Install the job on each node
      self._Locations().put(jobfilename, "/jobs/").collect()

      return hash

class FmLocalCommands(CommandSetBase):
   def __init__(self, args):
      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.add_option("-n", "--nodenum", help="Id number of this node")
      (options, cmd) = p.parse_args(args)

      if options.nodenum:
         os.environ['FMNODENUM'] = options.nodenum
      else:
         options.nodenum = os.environ['FMNODENUM']

      self.Locations = FmLocations(stdin=True)
      self.Locations.thisis(int(options.nodenum))

      try:
         CommandSetBase.__init__(self, cmd)
      except:
         traceback.print_exc()

   def restore(self, args):
      """[-s] files...

Iff -s is specified, then nodes should be chosen based on a pure
function of the file's suffix so that similarly split files will be
stored on the same nodes.  Also file will be stored into the same
directory in the virtual store that it originated in.
"""

      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.add_option("-s", "--suffix", action="store_true", help="Store based on file's suffix")
      (options, args) = p.parse_args(args)
   
      procs = Procs()
      n = None
      for f in args:
         dst = os.path.dirname(f[len(self.Locations.this.rootdir):]) + "/"
         
         if options.suffix: 
            ext = os.path.splitext(f)[1]
            n = ext

         # Pick some destinations.  If the algorithm picks this node,
         # that is reasonable, but is a NOOP
         dsts = self.Locations.pick(n, selfIsNoop=True)
         #if options.suffix: print >>sys.stderr, "Extension", ext, "to", [(x.hostname or "") + ":" + x.rootdir + "/" + dst for x in dsts.locs]
         dsts.forAll(["mkdir", "-p"], dst, glob=False)
         dsts.put(f, dst, procs=procs)

      procs.collect()

   def lsone(self, fname, fs):
 
      # Convert to ls-style human-readable permission mask
      perms = "?"
      for kind in "BLK", "CHR", "DIR", "LNK", "SOCK", "REG":
         if getattr(stat, "S_IS"+kind)(fs.st_mode):
            perms = kind[0].lower()
      if perms == "r": perms = "-"
         
      for who in "USR", "GRP", "OTH":
         for what in "R", "W", "X":
            if fs.st_mode & getattr(stat,"S_I"+what+who):
               perms=perms+what.lower()
            else:
               perms=perms+"-"

      # Construct the dict that is pickled and passed to the client
      d = {}
      d['name'] = fname
      d['node'] = self.Locations.thisnum
      d['perms'] = perms
      d['user'] = pwd.getpwuid(fs.st_uid).pw_name
      d['group'] = grp.getgrgid(fs.st_gid).gr_name
      d['size'] = fs.st_size
      d['mtime'] = fs.st_mtime
      return d

   def ls(self, args):
      os.chdir(self.Locations.this.rootdir)  
      #args = [(a.lstrip("/")).rstrip("/") for a in args] # Make relative; even if it started with a /
      procs = Procs()
      args = multiglob(["./" + a for a in args])
      if not args: return
      output = []
      for apath in args:

         # aname is the virtual name of the file; will be shown to user.
         # So strip the leading ./ that we added to make it relative
         aname = apath[2:].rstrip('/')

         s = os.stat(apath)
         if stat.S_ISDIR(s.st_mode):
            for f in os.listdir(apath):
               fpath = apath + "/" + f
               fname = aname + "/" + f
               output.append(self.lsone(fname, os.stat(fpath)))
         else:
            output.append(self.lsone(aname, s))
            
      sys.stdout.write(cPickle.dumps(output))
      return


   def sched(self, args):
      sockname = self.Locations.this.rootdir + "/.fmsock-" + str(os.getuid())
      s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)

      try:
         s.connect(sockname)
         ## Right now this is just for making sure a scheduler is running;
         ## We don't actually send anything over this socket (yet).
         #s.write(string.join(args) + "\n")
         #print >>sys.stderr, "Connected to scheduler"
         return

      except: 
         #print >>sys.stderr, "Warning", sys.exc_type, sys.exc_value, "; assuming scheduler not running yet"
         newname = sockname + "." + str(os.getpid())
         s.bind(newname)
         s.listen(1000)
         os.rename(newname, sockname)

      # If we get here, then we've bound as the (new) server

      # For now, the scheduler and FmJob still assume a global Locations var
      global Locations
      Locations = self.Locations

      s = JobScheduler(self.Locations.this)
      f = file(os.devnull)
      os.dup2(f.fileno(), 0)

      # daemonify
      if os.fork():
         return 

      os.setsid()
      if os.fork():
         return

      f = file(self.Locations.this.rootdir + "/.schedlog-" + str(os.getuid()), 'a')
      os.dup2(f.fileno(), 1)
      os.dup2(f.fileno(), 2)

      s.RunUntilDone()

   def wait(self, args):
      """Wait for a specified jobid to complete"""

      jobfile = self.Locations.this.rootdir+"/jobs/"+args[0]
      #print >>sys.stderr, "Waiting on", jobfile

      while os.path.isfile(jobfile):
         time.sleep(1)


def multiglob(globs):
   """Take a list of globs and return a list of all of the files that match any of those globs."""

   assert(type(globs) == type([]))
   ret = []
   for g in globs:
      ret += glob.glob(g)
   return ret

class SubprocessLs:
   def __init__(self, procs): 
      """Take a list of subprocesses"""
      self.procs = procs
      self.nextline = [None] * len(self.procs)

   def __iter__(self): 
      return self

   def next(self):
      smallest = None
      for i in range(0, len(self.procs)):
           # Job slot is set to None after that job terminates
           if not self.procs[i]: continue

           # Get next line from this job if we don't already have one cached
           if not self.nextline[i]:
              self.nextline[i] = self.procs[i].stdout.readline()

              # Check to see if the job terminated
              #if not self.nextline[i]:
                 #print >>sys.stderr, i, "exited", self.procs[i].poll()

           # See if this job's next line should go before any others
           if self.nextline[i] and (smallest == None or self.nextline[i] < self.nextline[smallest]):
              smallest = i

      if smallest == None:
            #print >>sys.stderr, "Done; left", self.nextline
            raise StopIteration

      s = self.nextline[smallest]
      self.nextline[smallest] = None
      return s 

FmCommands(sys.argv[1:])
