#!/usr/bin/python 
#
# FileMap - http://mfisk.github.com/filemap
#
# Public Domain License:
#
# This program was prepared by Los Alamos National Security, LLC at
# Los Alamos National Laboratory (LANL) under contract
# No. DE-AC52-06NA25396 with the U.S. Department of Energy (DOE). All
# rights in the program are reserved by the DOE and Los Alamos
# National Security, LLC.  Permission is granted to the public to copy
# and use this software without charge, provided that this Notice and
# any statement of authorship are reproduced on all copies.  Neither
# the U.S. Government nor LANS makes any warranty, express or implied,
# or assumes any liability or responsibility for the use of this
# software.
#
# Author: Mike Fisk <mfisk@lanl.gov>
#

import ConfigParser, sys, optparse, random, urlparse, subprocess, os, glob, errno, string, socket, time, traceback, sha, pydoc, tempfile, re, socket, cStringIO, base64, stat, cPickle, pwd, grp, shlex, select, signal, math

Verbose = 0

nonalphanumre = re.compile('\W')

def hash(s):
   h = sha.sha(s).digest()
   h = base64.b64encode(h, '+_')  # Normal b64 but / is _
   h = h.rstrip('\n\r =')
   return h

def isremote(hname):
   if not hname: return False

   me = socket.gethostname()
   me = me.split('.')[0]
   hname = hname.split('.')[0]
   if me == hname:
      return False
   else:
      return True

def escape(s):
   (s, num) = nonalphanumre.subn(lambda m: "=%02X" % ord(m.group()), s)
   return s

def rm_rf(path):
   for (dirpath,dirs,files) in os.walk(path, topdown=False):
      for f in files: 
         os.unlink(dirpath + "/" + f)

      os.rmdir(dirpath)

def mkdirexist(path):
   try:
      os.makedirs(path)
   except OSError, e:
      if e.errno == errno.EEXIST:
         pass
      else:
         raise

def rsyncDeWildcard(pth):
   """Take a path, potentially including wildcards, and reduce to rsync args that can't expand."""
   pth = pth.replace('//', '/').replace('/./','/')

   origpath = pth

   # Reduce pth to a non-wildcard path
   while '*' in pth or '?' in pth:
      pth = os.path.dirname(pth)
   if pth[-1] != "/": pth += "/"

   # Find the wildcard portion of the original path and make it a glob
   includes = [origpath[len(pth):].lstrip("/")]

   # Include all parents of the wildcard pattern
   x = includes[0]
   while True:
      dir = os.path.dirname(x)
      if dir == x: break
      x = dir
      includes.append(x + "/")

   includes.append(pth)

   return (includes, [pth])

def rsyncSimplify(lst):
   includes = []
   paths = []
   for l in lst:
         [newi, newp] = rsyncDeWildcard(l)
         includes += newi
         paths += newp
      
   includes = ["--include=" + x for x in includes] 
   includes.append("--exclude=*")
   return [includes, paths]
  
     
def close_list(seq):
   """Take a list of numbers like [1,2,3,5] and returns a list of ranges like ["1-3",5]"""
   result = []
   start  = None
   for i,item in enumerate(seq):
      if i < len(seq)-1 and item+1 == seq[i+1]:
          if start is None:
             start = item
      elif start is not None:
         result.append('%d-%d' % (start, item))
         start = None
      else:
         result.append(item)
   return result

def coallesce(outputs, labels):
   """Take a list of multi-line strings and an equally long list of labels and return a string summarizing which labels had what output."""
   assert(len(outputs) == len(labels))
   sameas = {}
   result = ""

   for i in range(0, len(outputs)):
      if outputs[i]:
         header = ""
         header = labels[i]
         for j in range(i+1, len(outputs)):
            if outputs[j] and outputs[i].getvalue() == outputs[j].getvalue():
               header += "," + labels[j]
               outputs[j] = None
         header += ": "
         for line in outputs[i]:
            result += header + line

   return result
 
class MethodOptionParser(optparse.OptionParser):
   """OptionParser to be instantiated by dispatch methods of a class.  Sets %prog to be the calling function name and epilog to be the doc string for that function.  Also prints options in usage string more like man conventions instead of just [options]."""

   def __init__(self, *args, **kwargs):
      if not kwargs.has_key('epilog'):
         caller = sys._getframe(1)
         pmethod = caller.f_code.co_name
         pself = caller.f_locals['self']
         method = pself.__class__.__dict__[pmethod]
         kwargs['prog'] = pmethod
         self.Epilog = method.__doc__ or ""

      if kwargs.has_key('fixed'):
         self.fixed = kwargs['fixed']
         del kwargs['fixed']
      else:
         self.fixed = None

      optparse.OptionParser.__init__(self, *args, **kwargs)

   def get_usage(self):
      if self.fixed != None:
         flags = []
         options = ''
         for o in self.option_list:
            if o.action == 'store_true':
               flags += o._short_opts[0].lstrip('-')
            elif o.action == 'help':
               pass
            else:
               options += "[%s %s]" % (o._short_opts[0], o._long_opts[0].lstrip('-').upper())
               if o.action == 'append':
                  options += '...'
               
               options += ' '

         flags = string.join(flags, '')
         if flags: options += "[-" + flags + "] "
         
         self.usage = "%prog " + options + self.fixed

      return optparse.OptionParser.get_usage(self)

   def print_help(self):
      r = optparse.OptionParser.print_help(self)
      
      # For python < 2.5, we have to print our own epilog 
      print
      print self.Epilog
      return r

class FmLocations:
   def __init__(self, config=None, locs=[], stdin=False):
      self.locs = locs # Allow pick() to derive an instance with a subset of the classes
      self.procs = Procs()
      self.cleanup = None

      if config:
         self.config = config

      else:
         self.config = ConfigParser.SafeConfigParser()

         if stdin:
            # Read config file from stdin
            self.config.readfp(sys.stdin)

            # Keep a copy of the config around so our children can reference it
            fd, name = tempfile.mkstemp(prefix="tmp-fm-locations-")
            self.cleanup = name
            os.environ['FMCONFIG'] = name
            fh = os.fdopen(fd, 'w')
            self.config.write(fh)
            fh.close()

         else:
            filename = os.environ.get('FMCONFIG')
            #print >>sys.stderr, "Env filename is", filename
            if not filename:
               if os.path.isfile("filemap.conf"):
                  filename = "filemap.conf"
               else:
                  filename = "/etc/filemap.conf"

            # Read config file from stdin
            r = self.config.read(filename)
            if not r:
               raise Exception("Unable to locate config file.  Set FMCONFIG or place filemap.conf in . or /etc")

            #print >>sys.stderr, "Reading config", filename, self.config.sections()
            os.environ['FMCONFIG'] = filename

         self.replication = int(self.config_get("global", "replication", "1"))
         self.umask = int(self.config_get("global", "umask", "0002"))
         os.umask(self.umask)

      # Unless we were passed an explicit list of locations, grab all
      # from the config file.
      if not locs:
         for s in self.config.sections():
            if s == "global": continue
            l = self.parseLocation(s)
            self.locs.append(l)

      # Use global config values by default (thisis() can override later)
      self.this = self.parseLocation("global")

   def __del__(self):
      if self.cleanup:
         os.unlink(self.cleanup)

   def parseLocation(self, stanza):
      l = FmLocation()
      l.name = stanza

      qbyhost = self.config.has_option("global", "queuebyhost")

      l.sshcmdstr = os.path.expanduser(self.config_get(stanza, "ssh", "ssh -o GSSAPIDelegateCredentials=yes -o ConnectTimeout=5 -o StrictHostKeyChecking=no -Ax", raw=True))
      l.sshcmd = shlex.split(l.sshcmdstr, comments=True)

      l.rsynccmd = shlex.split(self.config_get(stanza, "rsync", "rsync", raw=True), comments=True)
      l.rsynccmd[0] = os.path.expanduser(l.rsynccmd[0])

      if l.name != "global": 
         l.rootdir = os.path.expanduser(self.config_get(stanza, "rootdir"))
         l.hostname = self.config_get(stanza, "hostname")
         l.syncdir = os.path.expanduser(self.config_get(stanza, "syncdir", "/tmp/fmsync"))

         l.jobdir = l.rootdir + "/jobs"
         if qbyhost:
            l.qname = l.hostname
         else:
            l.qname = l.name

         pythoncmd = os.path.expanduser(self.config_get(stanza, "python", "python"))
         l.fmcmd = [pythoncmd, os.path.expanduser(self.config_get(stanza, "fm", l.rootdir + "/sys/fm"))]
         l.processes = int(self.config_get(stanza, "processes", "1"))

      return l
   
   def config_get(self, section, key, defval=None, raw=False):
      if self.config.has_option(section, key):
         return self.config.get(section, key, raw=raw)
      elif self.config.has_option("global", key):
         return self.config.get("global", key, raw=raw)
      else:
         return defval

   def thisis(self, n):
      # This sets of an easy to use member to get the info for this Location
      self.this = self.locs[n]
      self.thisnum = n

   def pickn(self, seed=None):
      """Pick some pseudo-random locations.  The number of locations
      chosen is based on the replication factor in the config.
      Specify a seed to get deterministic results (e.g. for a given
      extension number.)  The return value is the list of indices into
      the current locations."""

      #print "pick seed=", seed
      candidates = range(0, len(self.locs))
      used = []

      if seed: random.seed(seed)

      for i in range(0, self.replication):
         if not candidates:
            print >>sys.stderr, "Cannot pick %d locations from %d available" % (n, len(self.locs))
            break
         l = random.choice(candidates)
         used.append(l)
         candidates.remove(l)
     
      #print >>sys.stderr, "pick", seed, "->", used

      return used
         
   def pick(self, seed=None, selfIsNoop=False):
      """Return a new FmLocations instance that uses a subset of the
locations of this instance."""

      used = self.pickn(seed)
      if selfIsNoop and self.thisnum in used: 
         used.remove(self.thisnum)
      picked = FmLocations(self.config, [self.locs[i] for i in used])
      picked.this = self.this
      return picked
         
   def forAllLocal(self, cmd):
      """Same ase forAll(), but CMD will be run as arguments to "fm _local"."""

      lcmd = ["fm", "_local", "-n", "%(NODENUM)"] + cmd
      fp = cStringIO.StringIO()
      self.config.write(fp)

      return self.forAll(lcmd, subst=True, stdinstr=fp.getvalue(), tag=string.join(cmd))

   def forAll(self, Cmdv, src=[], dst=[], absolute=False, subst=False, trailer=None, glob=True, stdinstr=None, call=False, tag=None):
      if not tag: tag = string.join(Cmdv + src + dst)

      if src and type(src) != type([]): src = [src]
      if dst and type(dst) != type([]): dst = [dst]

      if stdinstr:
         stdin = subprocess.PIPE

      nodenum = 0
      for loc in self.locs:
         if subst: 
            cmd = [n.replace("%(ROOTDIR)", loc.rootdir).replace("%(SYNCDIR)", loc.syncdir).replace("%(NODENUM)", str(nodenum)) for n in Cmdv]
            src = [n.replace("%(SYNCDIR)", loc.syncdir) for n in src]
         else:
            cmd = list(Cmdv)

         if absolute:
            s = list(src)
            d = list(dst)
         else:
            s = [loc.rootdir + "/" + f for f in src]
            d = [loc.rootdir + "/" + f for f in dst]

         if cmd[0] == "fm":
            cmd = loc.fmcmd + cmd[1:]
         if isremote(loc.hostname):
            cmd = loc.sshcmd + [loc.hostname] + cmd + s + d

         else:
            if s and glob:
               #time.sleep(random.random())  #RHEL5 + thumper glob issues
               s = multiglob(s)
               if not s: continue

            if d and glob:
               d = multiglob(d)
               if not d: continue

            cmd += s + d 
            
         if trailer:
            cmd += trailer

         self.procs.Popen(cmd, call=call, stdinstr=stdinstr, queue=loc.qname, tag=tag)

         nodenum += 1

      return self.procs
   
   def put(self, src, dst, relative=False, mkdir=False, procs=None):
      if relative:
         # Make src path be relative root local rootdir and place in corresponding path under remote rootdir 
         rsynccmd = self.this.rsynccmd + ["-R"]
         root = self.this.rootdir
         os.chdir(root)
         assert(src[:len(root)] == root)
         src = src[len(root):].lstrip("/")
      elif mkdir:
         rsynccmd = self.this.rsynccmd + ["-R"]
      else:
         rsynccmd = self.this.rsynccmd

      s = multiglob([src])

      if not s:
         print >>sys.stderr, "No such file:", src
         return procs

      if not procs: procs = Procs()

      for loc in self.locs:
         if relative:
            d = loc.rootdir + "/"
         else:
            d = loc.rootdir + "/" + dst

         if isremote(loc.hostname):
            d = loc.hostname + ":" + d

         cmd = rsynccmd + ["-a", "-e", loc.sshcmdstr] + s + [d]

         procs.Popen(cmd, queue=loc.qname, tag="put %s %s" % (src, dst))

      return procs

   def get(self, args, procs=None, outfile=sys.stdout, relative=True):
      """Copy files from the cloud to local storage."""
      p = MethodOptionParser(fixed="src... dst")
      p.disable_interspersed_args()
      p.add_option("-c", "--cat", action="store_true", help="Cat files to stdout")
      p.add_option("-n", "--name", action="store_true", help="Show file name when catting files")
      (options, args) = p.parse_args(args)

      rsyncflags = "-rptgo"
      if relative: rsyncflags += "R"

      if options.cat:
         assert(len(args) > 0)
         dst = tempfile.mkdtemp(prefix="tmp-fm-get-") + "/"
         src = args
      else:
         assert(len(args) > 1)
         dst = args[-1]
         src = args[:-1]

      if not procs: procs = Procs()

      for loc in self.locs:
         # To make relative paths shorter in rsync >= 2.6.7 , put "/./" 
         # in the path between the rootdir and the relative path
         s = [loc.rootdir + "/./" + f for f in src]

         # Cleanup the path to encourage rsync to get "/./" magic right
         s = [i.replace('//','/') for i in s]

         #print loc.rootdir, src, s

         # Take the specified paths/globs and turn them into a list of simple (non-glob) paths and --include/exclude options
         [includes, s] = rsyncSimplify(s)

         if isremote(loc.hostname):
            s = [loc.hostname + ":" + string.join(s)]
         else:
            s = multiglob(s)
            if not s:
               #print >>sys.stderr, "No such file:", src
               continue

         cmd = loc.rsynccmd + includes
         cmd += [rsyncflags, "--prune-empty-dirs", "--copy-unsafe-links", "-e", loc.sshcmdstr] + s + [dst]

         procs.Popen(cmd, queue=loc.qname, tag="get %s" % (string.join(args)))

      procs.collect()

      if options.cat:
         empty = True
         for (dirpath, dirnames, filenames) in os.walk(dst):
            for f in filenames:
               fname = os.path.join(dirpath, f)
               if os.path.getsize(fname):
                  if options.name:
                     relname = fname[len(dst):]
                     print >>outfile, "=== %s ===" % relname
                     empty = False
                  outfile.writelines(file(fname))
         if options.name and not empty:
            print >>outfile, "======"

         rm_rf(dst)

      return procs

   def processes(self):
      return sum(i.processes for i in self.locs)
         
class FmLocation:
   def __init__(self):
      pass

class Procs:
   """This class is used to launch some number of child processes and reap them.
   It provides methods to instantiate a process for each node."""

   def __init__(self):
      self.queues = {}
      self.poll = select.poll()
      self.fd2proc = {}
      self.pids = {}
      self.numprocspolling = 0

   def isempty(self):
      return (not self.pids)

   def Popen(self, cmdv, **kwargs):
      kwargs['cmdv'] = cmdv
      queue = kwargs.get('queue')
      if not kwargs.get('tag'): kwargs['tag'] = string.join(cmdv)

      if queue and self.queues.get(queue):
         # Will process later
         if Verbose > 2: print >>sys.stderr, "Queueing", tag, "to", queue, len(self.queues[queue])
         self.queues[queue].append(kwargs)
         return
      else:
         # No queue, means we can run now, but create an empty queue to block subsequent tasks         
         self.queues[queue] = []
         return self.PopenNow(**kwargs)

   def PopenNow(self, cmdv=[], prefix=None, stderr=subprocess.PIPE, stdout=subprocess.PIPE, call=False, tag=None, queue=None, stdinstr=None, cwd=None):
      # Note, we prefer to have a stdout so that poll() can tell when it closes and then reap the child

      if stdinstr:
         stdin = subprocess.PIPE
      else:
         stdin = file('/dev/null','w')

      r = subprocess.Popen(cmdv, stdin=stdin, stdout=stdout, stderr=stderr, cwd=cwd)

      if r.stderr or r.stdout:
         self.numprocspolling += 1

         if r.stdout:
            self.poll.register(r.stdout, select.POLLIN|select.POLLHUP|select.POLLERR) 
            self.fd2proc[r.stdout.fileno()] = r
            r.stdoutbuf = cStringIO.StringIO()
   
         if r.stderr:
            self.poll.register(r.stderr, select.POLLIN|select.POLLHUP|select.POLLERR) 
            self.fd2proc[r.stderr.fileno()] = r
            r.stderrbuf = cStringIO.StringIO()

      r.tag = tag
      r.start = time.time()
      r.queue = queue
      self.pids[r.pid] = r
      if Verbose > 2: print >>sys.stderr, r.pid, cmdv, r.queue

      if stdinstr:
         r.stdin.write(stdinstr)
         r.stdin.close()

      return r

   def wakeup(self, qname): 
         #Wake-up anything on this queue
         if self.queues[qname]:
            kwargs = self.queues[qname].pop(0)
            #if not self.queues[qname]: 
               # If nothing left on queue, remove it so we won't block
               #print "remove queue"
               #del self.queues[qname]
            if Verbose > 2: print >>sys.stderr, "Waking", kwargs
            self.PopenNow(**kwargs)

   def collect(self, ignoreErrors=False, labelPrint=False):
      """Execute any active or queued processes and return, a boolean success scalar and a list of the processes objects that ran."""

      results = []
      errors = 0
      total = 0
      while True:
         p = self.waitpid(True)
         if p == None: break

         results.append(p)
         total += 1
         if p.status: errors += 1 
         if Verbose > 1:
            ec = os.WEXITSTATUS(p.status)
            print >>sys.stderr, "%s: %gs status %d for %s" % (p.queue, p.time, ec, p.tag)

      if Verbose > 0:
         print >>sys.stderr, "--- Report for", results[0].tag #XXX not al tags identical
         mean = 0.0
         for p in results:
            mean += p.time
        
         mean /= len(results)
         stddev = 0.0
         for p in results:
            stddev += (p.time - mean)**2

         stddev /= len(results)
         stddev = math.sqrt(stddev)

         if stddev > 0:
            for p in results:
               deviations = (p.time - mean) / stddev
               if deviations > 2:
                  print >>sys.stderr, "Node %s slow by %g standard deviations" % (p.queue, deviations)

            print >>sys.stderr, "Average time %gs, stddev %gs, for %s" % (mean, stddev, p.tag)

      for k,v in self.queues.items():
         assert(not v)
      assert(not self.pids)
      assert(self.numprocspolling == 0)
      self.poll = select.poll() #Reinit, just in case not everything was unregister()'d
         
      if not ignoreErrors and errors:
         print >>sys.stderr, "Error, %d/%d subprocess(es) returned error" % (errors, total)
            
      if labelPrint or not ignoreErrors:
         if labelPrint:
            sys.stderr.write(coallesce([f.stdout for f in results], [f.queue for f in results]))
         if not ignoreErrors:
            sys.stderr.write(coallesce([f.stderr for f in results], [f.queue for f in results]))

      if Verbose > 0:
         print >>sys.stderr, "---"

      return (errors == 0), results

   def finalize(self, p):
      # Record compute time
      after = os.times()
      p.time = time.time() - p.start
      p.utime = after[2] - p.starttimes[2]
      p.stime = after[3] - p.starttimes[3]
      del p.starttimes
      del p.start

      # Finish building the output buffers
      if p.stdout != None:
         self.poll.unregister(p.stdout)
         #print >>sys.stderr, "finalized", p.pid, p.stdout.fileno()
         del self.fd2proc[p.stdout.fileno()]
         p.stdoutbuf.write(p.stdout.read())
         p.stdout = p.stdoutbuf
         p.stdout.seek(0)

      if p.stderr != None:
         self.poll.unregister(p.stderr)
         del self.fd2proc[p.stderr.fileno()]
         p.stderrbuf.write(p.stderr.read())
         p.stderr = p.stderrbuf
         p.stderr.seek(0)

      if p.stderr or p.stdout:
         self.numprocspolling -= 1

      del self.pids[p.pid]

      self.wakeup(p.queue)
     
      return p


   def waitpid(self, block=False):
      """Check for any completed child, remove them from the procs dictionary and return them."""

      while block and not self.isempty(): 
         r = self.checkwait(block=block)
         if r: return r

      return None

   def checkwait(self, block=False):
      if block: 
         # First, the block argument is just a hint, we're not obligated to block.
         # Second, we may have children that are blocking on I/O to us, so we have use poll().
         # But we may have children that have no shared fd with us and that can only be reaped with waitpid().
         # So we need to figure out which case it is to determine where we can actually block.

         if self.numprocspolling == len(self.pids):
            # In this case, waitpid() is superfluous, so don't block on it, but
            # block on poll()
            waitopts = os.WNOHANG
            pollopts = -1 # Block
         elif self.numprocspolling:
            # In this case we have to bounce back between waitpid() and poll()
            # We don't want to have a complete busy loop, so we give poll a small timeout.
            # This creates some potential latency, but this is not a expected case
            print >>sys.stderr, "Debug: inefficient when only some children have fds we manage"
            waitopts = os.WNOHANG
            pollopts = 2500 #block
         else:
            # In this case poll() is superfluous and we should just do a blocking waitpid()
            waitopts = 0 #block
            pollopts = 0
      else:
         # Simple case
         pollopts = 0
         waitopts = os.WNOHANG

      #print >>sys.stderr, "block case", waitopts, pollopts

      # Check for completed children.
      try:
         # Waiting register's child's CPU times, so measure right before and after
         starttimes = os.times()

         (pid, status) = os.waitpid(0, waitopts) 
         if pid:
            p = self.pids[pid]
            p.status = status
            p.starttimes = starttimes
            return self.finalize(p)

      except OSError, e:
         if e.errno == errno.ECHILD:
            # If we're here its probably because in python 2.4, subprocess reaps other children
            # whenever making a new one.  So one of the children we thought was running had already
            # been waitpidded.  http://bugs.python.org/issue1731717
            # So, poll on each of our children to find the one to finalize:
            for p in self.pids.values():
               # First look for things reaped by the signal handler
               if p.__dict__.has_key('error'): return self.finalize(p)

               # Since the chils is already reaped, we'll probably measure no CPU time for it. oh well
               starttimes = os.times()
               status = p.poll()
               if status != None: 
                  p.status = status
                  return self.finalize(p)    

            print >>sys.stderr, "Unexpected: no child", self.pids
            raise
         else:
            raise

      # Look for children that may be blocking on writes to stdout, or that have finished
      # XXX: this won't catch children that we don't have share a fd with
      try:
         # Make sure we get interrupted if a child exits (in case we're not connected to any of its fds)
         l = self.poll.poll(pollopts)

      except select.error, e:
         if e[0] == errno.EINTR:
            return None
         else:
            raise
      else:
         for fd,event in l:
            p = self.fd2proc[fd]

            if event & select.POLLIN: 
               if fd == p.stdout.fileno():
                  s = p.stdout.read()
                  p.stdoutbuf.write(s)
               else:
                  assert(fd == p.stderr.fileno())
                  s = p.stderr.read()
                  p.stderrbuf.write(s)

      return None

# The design is as follows:
#   1. Jobs are submitted in an uncoordinated manner, so they have unique
#      identifiers that aren't generated by the user. but that are identical
#      across all nodes running a job. 
#   2. To manage the problem of listing jobs and removing jobs, we use a 
#      directory structure to contain all current jobs.  Deleting a file should
#      (eventually) lead to a job not running further.
#   3. New jobs should be invoked synchrously by calling a job scheduler with a
#      hint that points to the job.  Failure to send this hint should only delay
#      the job scheduler discovering the job since it should periodically poll
#      for changes in the job directory.
#   4. We assume out of band replication of the job dir across nodes (via NFS, 
#      rsync, etc.)

class JobScheduler:
   """JobScheduler maintains a set of child processes working on a set of jobs."""

   def __init__(self, options, numthreads=None):
      self.jobs = {}  # A dictionary of job objects indexed by job name
      self.procs = Procs()
      self.options = options

      self.freethreads = numthreads 
      if not self.freethreads:
         self.freethreads = self.options.processes

   def ReadJobDir(self):
      """Check for new/removed jobs"""

      jobs = os.listdir(self.options.jobdir)
      removed = set(self.jobs) - set(jobs)
      recent = set(jobs) - set(self.jobs)
      for j in removed:
         #print "Job", j, "removed"
         del self.jobs[j]

      for j in recent:
         #print "Job", j, "added"
         try:
            self.jobs[j] = FmJob(self.options.jobdir + "/" + j)
         except:
            traceback.print_exc()
            print >>sys.stderr, "Error parsing job description", j, "; skipping"

   def RunOnce(self):
         """Try to launch a work item and return True on success (None->nothing to do)"""

         self.freethreads -= 1

         # Look for something with work to do
         for job in self.jobs.keys(): #XXX, Should randomize order
            proc = self.jobs[job].compute(self.options, self.procs)
            #print >>sys.stderr, "Launched", proc
            if proc: 
               return True
            else:
               # No more work to do, check to see if job is complete
               # Note, job cannot be complete if parent is still active
               j = self.jobs[job]
               if not j.continuous and not j.running and (not j.parent or not self.jobs.has_key(j.parent)):
                  self.JobDone(job)

         self.freethreads += 1
         return None

   def JobDone(self, job):
      """A job has run to completion, remove it locally."""

      del self.jobs[job]
      #print >>sys.stderr, "Job", job, "done"
      os.unlink(self.options.jobdir + "/" + job)

   def RunUntilDone(self):
      """Run until there is no more input on it.  Return True iff we did something"""

      didSomething = False
      sleep = 0.001

      while True:
         if self.freethreads:
            self.ReadJobDir()
            if self.procs.isempty() and not len(self.jobs):
               # Doing nothing and no more jobs to run
               return didSomething
            if self.RunOnce():
               didSomething = True
               block=False
            else:
               # Nothing to do right now
               # Don't return because we're still working,
               # but wait before we look for more work to do.
               #print >>sys.stderr, "Wait for completion (and/or more work)"

               # Sleeping hurts latency for detecting new inputs, but avoids busy waits.
               # So we do an exponential backoff in how long we sleep with a max of 1 second
               sleep *= 10
               if sleep > 1: sleep = 1  # Max out at 1 sec sleep
               block=True
         else:
            block=True
            assert (not self.procs.isempty())

         p = self.procs.waitpid(block=block)
         #print >>sys.stderr, "WAITPID", block, p
         if p:
            self.freethreads += 1
            self.finalize(p)

   def finalize(self, p):
      stats = {}
      stats['status'] = p.status
      stats['time'] = p.time
      stats['utime'] = p.utime
      stats['stime'] = p.stime
      stats['inputsize'] = p.inputsize
      stats['nodename'] = self.options.name
      stats['hostname'] = socket.gethostname()
      stats['schedpid'] = os.getpid()

      cPickle.dump(stats, p.syncfile)
      p.syncfile.close()
      p.syncfile = None

      f = file(p.outdirname + "/." + p.outbasename + "/status", "w")
      cPickle.dump(stats, f)
      f.close()
   
      dst = p.outdirname + "/" + p.outbasename 
      if os.path.exists(dst): 
          rm_rf(dst)
          # By removing this dst, we're invalidating any previously derived data, but our redo-if-newer-than logic will handle the compute.

      os.rename(p.outdirname + "/." + p.outbasename, dst)
      p.job.running -= 1 

      del p
     
class FmJob:
   """Each FmJob object represents a job, as specified in a job file, and
    provides methods for identifying and processing inputs for that job."""

   def __init__(self, fname):
      config = ConfigParser.SafeConfigParser()

      processed = config.read(fname)
      if not processed:  
         raise IOError(errno.ENOENT, "Could not open file", fname)

      self.jobname = os.path.basename(fname)
      self.cmd = config.get("mrjob", "cmd", raw=True)
      self.inputs = config.get("mrjob", "inputs")
      self.continuous = config.has_option("mrjob", "continuous")
      self.reduce = config.has_option("mrjob", "reduce")
      if config.has_option("mrjob", "parent"):
         self.parent = config.get("mrjob", "parent")
      else:
         self.parent = None

      # Number of threads currently processing data for this job.
      self.running = 0

   def compute(self, options, procs):
      inputs = multiglob([options.rootdir + "/" + x for x in self.inputs.split()])

      if self.reduce:
         exts = [os.path.basename(i) for i in inputs]
         exts = list(set(exts))  # Get unique extensions

         for ext in exts:
            # Don't process .d directories as named inputs (feedback loop)
            if ext[-2:] == ".d": continue

            # Apply all of the files with the same extension,
            # but only if that extension belongs on this node.
            choices = Locations.pickn(ext)
            if Locations.thisnum not in choices:
               #print >>sys.stderr, Locations.thisnum, "hashes", ext, "to", choices
               continue

            #Get list of files with this extension
            xlen = len(ext)
            files = []
            for i in inputs:
               if i[-xlen:] == ext:
                  files.append(i)

            outfilename = "/reduce/" + self.jobname + "." + ext

            p = self.computeItem(files, outfilename, options, procs)
            if p: 
               #print >>sys.stderr, "Reduce ext", ext, "on node", Locations.thisnum, files, "of", self.inputs

               return p

         return None # Nothing to do

      for i in inputs:
         if i[-2:] == ".d": 
            # Don't process .d directories as named inputs (feedback loop)
            continue

         bname = os.path.basename(i)

         relativename = i[len(options.rootdir):]
         outfilename = relativename + ".d/" + escape(self.cmd)
        
         p = self.computeItem(i, outfilename, options, procs)
         if p: return p

      return None # Nothing to do

   def computeItem(self, inputs, outfilename, options, procs):
      """Start (but don't wait for completion) running this job on of the next unprocessed input for this job."""

      if type(inputs) != type([]):
         inputs = [inputs]

      syncfilename = options.syncdir + "/" + outfilename + "/status"
      mkdirexist(os.path.dirname(syncfilename))
      try:
         syncfile = os.fdopen(os.open(syncfilename, os.O_CREAT|os.O_SYNC|os.O_EXCL|os.O_WRONLY), "w")
      except OSError, e:
         if e.errno == errno.EEXIST:
            # This is a common case; somebody has already grabbed
            # the synchronization file, so we don't need to process
            # this file --- unless in the inputs are newer than the outputs
            try:
               lasttime = os.path.getmtime(syncfilename)
               redo = False
               for i in inputs:
                  if os.path.getmtime(i) > lasttime:
                     redo = True
                     break
            except: 
               traceback.print_exc()
               print >>sys.stderr, "Skipping file", outfilename
               return None

            if not redo:
               return None
               
            # Nuke the syncfile
            try:
               os.unlink(syncfilename)
               syncfile = os.fdopen(os.open(syncfilename, os.O_CREAT|os.O_EXCL|os.O_WRONLY), "w")
            except:
               # Somebody else was doing this at the same time, let them have it
               return None
            
         else:
            raise

      # Now we hold the "lock" on the syncfile, so proceed...

      print >>syncfile, socket.gethostname(), os.getpid()
      syncfile.flush()

      obase = options.rootdir + "/" + outfilename

      # Now insert a "." at the beginning of the basename for partial output
      odirname = os.path.dirname(obase)
      obasename = os.path.basename(obase)
      obase = odirname + "/." + obasename

      oname = obase + "/0"
      ename = obase + "/stderr"
      mkdirexist(obase)
      sout = os.fdopen(os.open(oname, os.O_CREAT|os.O_WRONLY|os.O_TRUNC), "w")
      serr = os.fdopen(os.open(ename, os.O_CREAT|os.O_WRONLY|os.O_TRUNC), "w")

      cmd = self.cmd

      if '%(input)' in cmd:
         cmd = shlex.split(cmd)
         try:
             # In this case, %(input) is just an argument and can be expanded
             iidx = cmd.index('%(input)')
             cmd[iidx:iidx+1] = inputs
         except ValueError:
             # If we're here, then when know %(input) is in some argument to
             # something else.  We must find it and replace it with some nice
             # spaces.
             for i in range(len(cmd)):
                 if not '%(input)' in cmd[i]:
                    continue
                 cmd[i] = cmd[i].replace('%(input)',' '.join(inputs),1)
                 break
      else:
         #cmd = cmd.split() + inputs
         cmd = shlex.split(cmd) + inputs
    
      if cmd[0] == "fm":
         cmd = options.fmcmd + cmd[1:]

      cmd = [os.path.expanduser(c) for c in cmd]
 
      try:
         p = procs.Popen(cmd, stdout=sout, stderr=serr, cwd=obase)
      except:
         #print >>serr, "Error", sys.exc_value, "executing", cmd
         print >>sys.stderr, "Error", sys.exc_value, "executing", cmd
         raise

      assert(p)
      #print >>sys.stderr, "JOB RUNNING", self.jobname, p, procs.pids
      p.inputsize = 0
      for i in inputs:
         p.inputsize += os.path.getsize(i)
      p.syncfile = syncfile
      p.outdirname = odirname
      p.outbasename = obasename
      p.job = self
      self.running += 1

      sout.close()
      serr.close()

      #print >>sys.stderr, "computeItem", cmd, socket.gethostname(), p.pid
      return p

class CommandSetBase(object):
   """This is a base-class for defining methods which are used as
   command line-based commands.  You should inherit from it and define
   methods.  All methods will be callable.  Usage and help information
   will only include methods that do not begin with an underscore."""

   def __method(self, mname):
      return self.__class__.__dict__[mname]

   def _usage(self):
      """Return a multi-line usage string based on the defined methods and
their doc strings."""

      usage = 'For more help, specify one of the following commands followed by -h option:\n'
      keys = self.__class__.__dict__.keys()
      keys.sort()
      for cmd in keys:
         if cmd[0] == "_": continue
         method = self.__method(cmd)
         #docstr = pydoc.text.indent(pydoc.text.document(method)).replace("(self, args)","")
         docstr = method.__doc__
         if docstr:
            docstr = docstr.split("\n")[0].strip().rstrip(".")
         usage += "   %-8s - %s\n" % (cmd, docstr)
      return usage

   def __init__(self, args, optParser=None):
      if not optParser:  
         optParser = MethodOptionParser()
         optParser.set_usage("""%prog command args...\n""" + self._usage())
         optParser.disable_interspersed_args()
      (options, args) = optParser.parse_args(args)
   
      if not args: 
         optParser.print_help()
         exit(1)

      try:
         method = self.__method(args[0])

      except:
         print >>sys.stderr, "Unknown command", args[0]
         optParser.print_help()
         exit(1)

      method(self, args[1:])

   def _optionHook(self, optParser):
      pass

class FmCommands(CommandSetBase):
   def __init__(self, args):
      """FileMap is a file-based map-reduce system.  
You can think of it is parallel and distributed xargs or make.  
It features replicated storage and intermediate result caching.

http://mfisk.github.com/filemap
"""

      self._locs = None
      p = MethodOptionParser()
      p.set_usage("""fm command args...\n""" + self._usage())
      p.add_option("-v", "--verbose", action="count", help="Increase verbosity (can be used multiple times)")
      p.disable_interspersed_args()
      (options, args) = p.parse_args(args)

      if options.verbose:
         global Verbose
         Verbose += options.verbose

      CommandSetBase.__init__(self, args, p)

   def _Locations(self):
      if not self._locs: 
         self._locs = FmLocations()
      return self._locs

   def split(self, args):
      """Partition an input file.
Split an inputfile into n pieces.  By default, the first
whitespace-delimited field is used as the key.  All lines with the
same key will be placed in the same output file.  The -r option can be
used to specify a Perl-compatible regular expression that matches the
key.  If the regex contains a group, then what matches the group is
the key; otherwise, the whole match is the key.

The root of the output file names must be specified either on the
command line or in the FMOUTPUT environment variable (which is set for
all programs running as FM jobs).  
"""
      p = MethodOptionParser(fixed="infile")
      p.add_option("-n", "--nways", help="Number of output files to use")
      p.add_option("-r", "--regex", help="Regex that matches the key portion of input lines")
      (options, args) = p.parse_args(args)
      options.nways = int(options.nways)

      assert(len(args))
      infile = args[0]

      #print >>sys.stderr, "Writing to", ofile

      if not options.nways:
         print >>sys.stderr, "-n option required"
         return False
      
      if not options.regex:
         options.regex = '^([^\s]*)'
         
      options.regex = re.compile(options.regex)

      files = []
      for i in range(0, options.nways):
         fname = str(i+1)
         files.append(file(fname, "w"))

      for line in file(infile):
         key = options.regex.search(line)
         if key:
            g = key.groups()
            if len(g):
               key = g[0]
            else:
               key = key.group(0)
         else:
            print >>sys.stderr, "Key not found in line:", line.rstrip()

         i = hash(key) % options.nways
         files[i].write(line)

      for f in files: f.close()

   def kill(self, args):
      """Kill a job."""
      (options, args) = MethodOptionParser(fixed="jobid").parse_args(args)
      if len(args) < 1:
         print >>sys.stderr, "Must specify a JobId to kill"
         return False
      args = ["/jobs/" + a for a in args]
      self._Locations().forAll(["rm", "-f"], args).collect()

   def mv(self, args):
      """Rename files in the cloud"""
      (options, args) = MethodOptionParser(fixed="src... dst").parse_args(args)
      if len(args) < 2:
         print >>sys.stderr, "mv requires at least 2 arguments"
         return False

      dst = args[-1]
      each = args[:-1]
      if len(each) > 1: dst += "/"
 
      self._Locations().forAll(["mv"], each, dst).collect()

   def mkdir(self, args, absolutes=[], async=False):
      """Make a directory (or directories) on all nodes.  
Has unix "mkdir -p" semantics."""
      (options, args) = MethodOptionParser(fixed="dir...").parse_args(args)
      # Make sure destination exists
      p = self._Locations().forAll(["mkdir", "-p", "-m", "2775"] + absolutes, args, subst=True, glob=False)
      if not async: p.collect()

   def jobs(self, args):
      """Show all of the jobs still active."""
      (options, args) = MethodOptionParser(fixed="").parse_args(args)

      tmpdir = tempfile.mkdtemp(prefix='tmp-fm-jobs-')

      self._Locations().get(["/jobs/*", tmpdir], relative=False)
      fmt = "%-28s %-15s %s"
      print fmt % ("Job ID","Command", "Inputs")
        
      for f in os.listdir(tmpdir):
         j = FmJob(os.path.join(tmpdir, f))
         print fmt % (f, j.cmd, j.inputs)

      rm_rf(tmpdir)
  
   def df(self, args):
      """Show free space on each node."""
      (options, args) = MethodOptionParser(fixed="").parse_args(args)

      self._Locations().forAll(["df", "-h"], ["/"]).collect(labelPrint=True)

   def chmod(self, args, absolutes=[], ignoreErrors=False):
      """Change permissions on files in the cloud."""
      (options, args) = MethodOptionParser(fixed="mode files...").parse_args(args)
      self._Locations().forAll(["chmod", args[0]] + absolutes, args[1:], subst=True).collect(ignoreErrors=ignoreErrors)

   def chgrp(self, args, absolutes=[]):
      """Change GID on files in the cloud."""
      (options, args) = MethodOptionParser(fixed="perm files...").parse_args(args)
      self._Locations().forAll(["chgrp", args[0]] + absolutes, args[1:], subst=True).collect()

   def store(self, args):
      """Store one or more files into the cloud."""

      """src... dst

Copy the specified file(s) into the virtual store.  
"""
      (options, args) = MethodOptionParser(fixed="files... dst").parse_args(args)

      assert(len(args) > 1)
      dst = args[-1]
      args = args[:-1]
      if len(args) > 1: dst += "/"

      procs = Procs()

      # We always specify a seed when calling pick() so there is stability in which nodes work is mapped to.
      # It's nice if they go to nodes that might have already received the store in a previous execution.
      for a in args:
         # Convert directory arguments to lists of files
         if os.path.isdir(a):
             for root,dirs,files in os.walk(a):
                for f in files:
                   fp = os.path.join(root, f)
                   dstfile = os.path.join(dst, root[0:]) + "/" + os.path.basename(fp)
                   # Build up dstfile for pick(), but since we're doing mkdir=True, just send dst to put()
                   self._Locations().pick(seed=dstfile).put(fp, dst, procs=procs, mkdir=True)
         else:
            dstfile = dst + "/" + os.path.basename(a)
            self._Locations().pick(seed=dstfile).put(a, dstfile, procs=procs)
            
      procs.collect()

   def map(self, args):
      """Launch a computation on a set of input files in the cloud.
Run the specified command on each input file (in the virtual store)
described by the inputglob.  Multiple inputglob arguments can be
given.  The -c option says that the commond should continue to run on
new inputs as they arrive.

Multiple commands can be chained together with |.  Each output file of
the first command becomes an input for the next command in the
pipeline.

The -f option says that any previously cached output should be ignored
and the program re-run.
"""

      p = MethodOptionParser(fixed="cmd [| cmd...]")
      p.disable_interspersed_args()
      p.add_option("-i", "--inputglob", action="append", help="Glob of input files (in root)")
      p.add_option("-c", "--continuous", action="store_true", help="Continue to look for new input files to arrive")
      p.add_option("-f", "--fresh", action="store_true", help="Do not use any cached output")
      p.add_option("-q", "--quiet", action="store_true", help="Do not fetch and display results")
      (options, cmd) = p.parse_args(args)

      if not options.inputglob:
         print >>sys.stderr, "Error: -i or --inputglob must be specified"
         return None

      cmds = string.join(cmd).split("|")
      inglobs = options.inputglob
      outglobs = []
      assert (type(inglobs) == type([]))

      tmpdir = tempfile.mkdtemp(prefix="tmp-fm-map-") + "/"

      jobname = None
      for cmd in cmds:
         reduce = False
         cmd = cmd.strip()
         if cmd[0] == ">":
            cmd = cmd.lstrip(">")

            # First, re-distribute the data
            restore = "fm _local restore -s" 
            jobname, ignore = self._MapComponent(restore, inglobs, outglobs, options, parent=jobname, tmpdir=tmpdir)
            # Ignore change to the inglobs directory since restore puts files in the same virtual path they started in

            # Barrier; Wait for that job (and its parents) to finish:
            ## Install the jobs on each node
            self._Locations().put(tmpdir + "*", "/jobs/").collect()

            print >>sys.stderr, "Waiting for distribute to complete before reducing.  Job", jobname
            self._Locations().forAllLocal(["wait", jobname]).collect()

            reduce = True

         jobname, inglobs = self._MapComponent(cmd, inglobs, outglobs, options, reduce=reduce, parent=jobname, tmpdir=tmpdir)


      # Install the jobs on each node
      self._Locations().put(tmpdir + "*", "/jobs/").collect()

      rm_rf(tmpdir)
      wallclock = time.time()

      print >>sys.stderr, "Waiting for completion.  Job", jobname
      self._Locations().forAllLocal(["wait", jobname]).collect()

      wallclock = time.time() - wallclock

      # Print any stderr files to stderr

      # Get the output files
      outputglobs = [f + "/stderr" for f in outglobs]
      outputglobs += [f + "/status" for f in outglobs]
      if not options.quiet:
         outputglobs += inglobs
      loc = tempfile.mkdtemp(prefix="tmp-fm-status-")
      self.get(outputglobs + [loc])

      # Process the output files
      wtime = 0.0
      utime = 0.0
      stime = 0.0
      bytes = 0.0
      errcodes = {}
      nodewtime = {}
      nodebytes = {}
      for (dirpath, dirnames, filenames) in os.walk(loc):
         for f in filenames:
            fname = os.path.join(dirpath, f)
            fh = file(fname)

            if f == "status":
               # Tally up info from status files
               stats = cPickle.load(fh)
               ec = os.WEXITSTATUS(stats['status'])
               errcodes[ec] = errcodes.get(ec, 0) + 1
               wtime += stats['time']
               utime += stats['utime']
               stime += stats['stime']
               bytes += stats['inputsize']
               nodename = stats['nodename']
               nodewtime[nodename] = nodewtime.get(nodename, 0) + stats['time']
               nodebytes[nodename] = nodebytes.get(nodename, 0) + stats['inputsize']

            elif f == "stderr":
               # Do nothing if empty
               if os.path.getsize(fname):
                  print >>sys.stderr, '===', fname, '==='
                  sys.stderr.writelines(fh)
                  print >>sys.stderr, '====='
            elif f == "0":
               sys.stdout.writelines(fh)
               
      rm_rf(loc)

      # Print out summary info
      errstr = ''
      for k in errcodes.keys():
         if not errstr: 
            errstr = "%d processes returned %d" % (errcodes[k], k)
         else:
            errstr += "; %d x %s" % (errcodes[k], k)
      if not errstr: errstr = 'No processes ran'
      MB = bytes/(2**20)

      if Verbose > 0:
         for n in nodebytes:
            print >>sys.stderr, "Node %s %g MB/s" % (n, nodebytes[n]/2**20/nodewtime[n])

      nodes = self._Locations().processes()
      if nodes and wallclock:
         print >>sys.stderr, errstr, "in %g seconds, %.0f%% User, %.0f%% System, %.0f%% Wait, %.0f%% Idle, %.1fx CPU scaling, %g MB/s" \
            % (wallclock, 100*utime/nodes/wallclock, 100*stime/nodes/wallclock, 100*(wtime - stime - utime)/nodes/wallclock, 100*(1 - wtime/nodes/wallclock), (stime+utime)/wallclock, MB/wallclock)
 
   def get(self, *args, **kwargs):
      """Copy one or more files from the cloud ot local storage."""
      return self._Locations().get(*args, **kwargs)

   def run(self, args):
      """Run a command on each node and print results."""
      (options, args) = MethodOptionParser(fixed="cmd...").parse_args(args)
  
      self._Locations().forAll(args, subst=True).collect(labelPrint=True)

   def cat(self, args):
      """Concatenate one or more files to stdout."""
      (options, args) = MethodOptionParser(fixed="files...").parse_args(args)

      procs = self._Locations().forAll(["cat"], args)
      for f in procs.pids.values():
            sys.stdout.writelines(f.stdout)
      return procs.collect(ignoreErrors=True)

   def _local(self, args):
      """This is for slave (remote) instantiations."""
      FmLocalCommands(args)

   def ls(self, args):
      """File & directory listing."""
      (options, args) = MethodOptionParser(fixed="files...").parse_args(args)

      if not args: args = ["/"]
      procs = self._Locations().forAllLocal(["ls"] + args)
      status, ps = procs.collect()
      ls = {}
      for p in ps:
         pkl = p.stdout.read()
         p.stdout.close()
         if not pkl: continue

         try:
            files = cPickle.loads(pkl)
         except:
            print >>sys.stderr, "Error", sys.exc_value, "parsing remote ls results:", pkl
            return -1

         for f in files:
            fname = f['name']
            if not ls.has_key(fname): ls[fname] = []
            ls[fname] += [f]

      keys = ls.keys()
      keys.sort()
      for k in keys:
         base = ls[k][0]
         nodes = [base['node']]
         isdir = (base['perms'][0] == 'd')
         islnk = (base['perms'][0] == 'l')
         criteria = ['perms', 'user', 'group', 'size']
         if isdir: base['size'] = 0
         if not isdir: criteria.append('size')
        
         # Look for duplicates from other nodes 
         for i in ls[k][1:]:
            if isdir: i['size'] = 0

            # See if this node's instance matches the right criteria to be
            # collapsed into a single output line
            match = True
            for a in criteria:
               if base[a] != i[a]: 
                  # Does not match
                  match = False
                  break

            if match:
               # If you specify the same file twice, we don't want
               # the node number(s) duplicated in the node list
               if i['node'] not in nodes:
                  nodes.append(i['node'])
            else:
               # No match, so output a line for this instance
               print self._lsline(i)

         # Update the nodelist for this base instance and print it out
         base['node'] = nodes
         print self._lsline(base)
         
      return 

   def _lsline(self, attrs):
      size = "%-.2g" % attrs['size']
      size = size.replace("e+", "e").replace("e0", "e")

      nodes = attrs['node']
      if type(nodes) != type([]): nodes = [nodes]

      if len(nodes) > 1:
         attrs['mtime'] = '-'
      else:
         lt = time.localtime(attrs['mtime'])
         if lt[0] == time.localtime()[0]:
            attrs['mtime'] = time.strftime("%b %d %H:%M", lt)
         else:
            attrs['mtime'] = time.strftime("%b %d  %Y", lt)
               
      if len(nodes) == len(self._Locations().locs):
         nodes = '*'
      else:
         nodes.sort()
         nodes = close_list(nodes)
         nodes = [str(i) for i in nodes]
         nodes = string.join(nodes,',')

      return "%s %7s %8s %8s %6s %12s %s" % (attrs['perms'], nodes, attrs['user'], attrs['group'], size, attrs['mtime'], attrs['name'])

   def rm(self, args):
      """Delete a file or directory tree in the cloud.
Remove empty file or directory.  Normal rm flags are passed through; -f is implied.
"""
      p = MethodOptionParser(fixed="files...")
      p.add_option('-R', action='store_true', help="recurse")
      (options, args) = p.parse_args(args)
      if options.R:
         options = ["-R"]
      else:
         options = []

      self._Locations().forAll(["rm", "-f"] + options, args).collect()


   def init(self, args):
      """Create/correct directory structure.
 If a groupname is specified, it will be applied to the directories."""
      (options, args) = MethodOptionParser(fixed="[group]").parse_args(args)

      self.mkdir(["/", "/jobs", "/reduce", "/sys"], absolutes=["%(SYNCDIR)"], async=True)
      if args:
         self.chgrp([args[0], "/", "/jobs", "/reduce", "/sys"], absolutes=["%(SYNCDIR)"])
      self.chmod(["g+rwxs", "/", "/jobs", "/reduce"], absolutes=["%(SYNCDIR)"], ignoreErrors=True)
      self._Locations().put(__file__, "/sys/fm").collect()

   def _MapComponent(self, cmd, inglobs, outglobs, options, reduce=False, parent=None, tmpdir="/tmp/"):
      jobdesc = "[mrjob]\ncmd = %s\ninputs = %s\n" % (cmd, string.join(inglobs))
      if parent: jobdesc += "parent = %s\n" % (parent)

      if options.continuous:
         jobdesc += "continuous = True\n"

      if reduce:
         jobdesc += "reduce = True\n"

      h = hash(jobdesc)
      jobfilename = tmpdir + h
      jobfile = open(jobfilename, "w")
      jobfile.write(jobdesc)
      jobfile.close()

      # Each component of pipeline uses previous output as input
      if reduce:
         newinglobs = ["/reduce/" + h + ".[0-9]*"]
         outglobs += newinglobs
      else:
         newinglobs = [i + ".d/" + escape(cmd) for i in inglobs]
         outglobs += newinglobs

      if options.fresh:
         #XXX lower latency to accumulate these up to map() and let it do a bulk delete

         if reduce:
            syncfiles = ["%(SYNCDIR)/reduce/" + h + "/[0-9]*"]
         else:
            syncfiles = ["%(SYNCDIR)/" + g + ".d/" + escape(cmd) + "/status" for g in inglobs] 

         # Delete sync files so that computation is forced to (re-)execute
         # These files are shared, so we pick a minimum number of nodes to do this on
         self._Locations().pick().forAll(["rm", "-f"], syncfiles, absolute=True, subst=True).collect()

         # Also delete previous output so that it doesn't keep getting used
         # (In the case of replication, a node may not replace the old output with new output
         self._Locations().forAll(["rm", "-Rf"], newinglobs).collect()

         # Only force the first stage of the pipeline.  Out-of-date checks will handle downstream.
         options.fresh = False

      newinglobs = [i + "/[0-9]*" for i in newinglobs]
      return h, newinglobs

class SchedLock:
   """Traditional daemon-style lock as a file containing the pid of the holder"""
   def __init__(self, rootdir):
      self.lockname = rootdir + "/.fmschedlock-" + str(os.getuid())

   def checkpid(self):
      # Check for running
      try:
         f = file(self.lockname)
      except:
         return False

      pid = f.read().strip()
      if not pid: 
         print >>sys.stderr, "Warning, ignoring empty lockfile", self.lockname
         return False
      pid = int(pid)

      try:
          # This will do nothing, but make sure the proc is running and ours
          os.kill(pid, 0)

          # If we got here, then the process exists, so give up
          return pid
      except OSError, e:
          if e.errno == errno.ESRCH:
             # process not running
             return None

          else:
             # Some other unexpected error
             raise
      
   def lock(self):
      try:
         lockfile = os.fdopen(os.open(self.lockname, os.O_CREAT|os.O_SYNC|os.O_EXCL|os.O_WRONLY), "w")
         lockfile.write(str(os.getpid()))
         lockfile.close()
         return True

      except OSError, e:
         if e.errno == errno.EEXIST:
            # Check for running
            if self.checkpid():
               return False
            else:
               # Stale, so Steal
               tmp = self.lockname + "-" + str(os.getpid())
               try:
                  os.unlink(self.lockname)
               except:
                  # Somebody beat us
                  return False
               
               # XXX race here if 2 procs competing to steal and 1 unlinks and recreates before 2nd unlinks
               time.sleep(1)

               f = file(tmp, "w")
               f.write(str(os.getpid()))
               f.close()
               try:
                  os.rename(tmp, self.lockname)
                  return True
               except OSError, e:
                  if e.errno == errno.EEXIST:
                     # Somebody beat us
                     return False
                  else:
                     raise
         else:
            raise

   def unlock(self):
      #XXX: doesn't check that we still hold the lock
      os.unlink(self.lockname)
      
class FmLocalCommands(CommandSetBase):
   def __init__(self, args):
      self._locs = None
      p = MethodOptionParser()
      p.set_usage("""fm _local command args...\n""" + self._usage())
      p.add_option("-n", "--nodenum", help="Id number of this node")
      p.disable_interspersed_args()
      (options, args) = p.parse_args(args)

      if options.nodenum:
         # If -n is given, then we are a direct slave and 
         # should get our config on stdin
         os.environ['FMNODENUM'] = options.nodenum
         self.Locations = FmLocations(stdin=True)
      else:
         # We are a child of a scheduler and
         # should pick-up the config from an environment variable
         options.nodenum = os.environ['FMNODENUM']
         self.Locations = FmLocations()

      self.Locations.thisis(int(options.nodenum))

      CommandSetBase.__init__(self, args, p)

   def restore(self, args):
      """[-s] files...

Iff -s is specified, then nodes should be chosen based on a pure
function of the file's suffix so that similarly split files will be
stored on the same nodes.  Also file will be stored into the same
directory in the virtual store that it originated in.
"""

      p = MethodOptionParser()
      p.disable_interspersed_args()
      p.add_option("-s", "--suffix", action="store_true", help="Store based on file's suffix")
      (options, args) = p.parse_args(args)

      procs = Procs()
      n = None
      for f in args:
         dst = os.path.dirname(f[len(self.Locations.this.rootdir):]) + "/"
         
         if options.suffix: 
            ext = os.path.basename(f)
            n = ext

         # Pick some destinations.  If the algorithm picks this node,
         # that is reasonable, but is a NOOP
         dsts = self.Locations.pick(n, selfIsNoop=True)
         #if options.suffix: print >>sys.stderr, "Extension", ext, "to", [(x.hostname or "") + ":" + x.rootdir + "/" + dst for x in dsts.locs]

         # Store with relative=True to make parent directories
         dsts.put(f, dst, relative=True, procs=procs)

      procs.collect()

   def lscharmod(self, perms, offset, chr):
      perms = list(perms)
      if perms[offset] == "-": 
           perms[offset] = chr.upper()
      else:
           perms[offset] = chr.lower()
      return string.join(perms, '')

   def lsone(self, fname, fs):
      # Construct the dict that is pickled and passed to the client
      d = {}
      d['name'] = fname
      d['node'] = self.Locations.thisnum

      if not fs:
          d['user'] = '?'
          d['group'] = '?'
          d['size'] = -1
          d['mtime'] = 0
          d['perms'] = '----------'

          # No stat structure
          return d
      
      # Convert to ls-style human-readable permission mask
      perms = "?"
      for kind in "BLK", "CHR", "DIR", "LNK", "SOCK", "FIFO", "REG":
         if getattr(stat, "S_IS"+kind)(fs.st_mode):
            perms = kind[0].lower().replace("f","p")
      if perms == "r": perms = "-"
         
      for who in "USR", "GRP", "OTH":
         for what in "R", "W", "X":
            if fs.st_mode & getattr(stat,"S_I"+what+who):
               c = what.lower()
            else:
               c = "-"
            perms += c

      if stat.S_ISUID & fs.st_mode: perms = self.lscharmod(perms, 3, "s")
      if stat.S_ISGID & fs.st_mode: perms = self.lscharmod(perms, 6, "s")
      if stat.S_ISVTX & fs.st_mode: perms = self.lscharmod(perms, 9, "t")
 
      d['perms'] = perms
      d['size'] = fs.st_size
      d['mtime'] = fs.st_mtime

      try:
         d['user'] = pwd.getpwuid(fs.st_uid).pw_name
      except:
         d['user'] = str(fs.st_uid)

      try:
         d['group'] = grp.getgrgid(fs.st_gid).gr_name
      except:
         d['group'] = fs.st_gid

      return d

   def ls(self, args):
      os.chdir(self.Locations.this.rootdir)  
      #args = [(a.lstrip("/")).rstrip("/") for a in args] # Make relative; even if it started with a /
      procs = Procs()
      args = multiglob(["./" + a for a in args])
      if not args: return
      output = []
      for apath in args:

         # aname is the virtual name of the file; will be shown to user.
         # So strip the leading ./ that we added to make it relative
         aname = apath[2:].rstrip('/')

         s = os.stat(apath)
         if stat.S_ISDIR(s.st_mode):
            for f in os.listdir(apath):
               fpath = apath + "/" + f
               fname = aname + "/" + f
               s = None
               try:
                  s = os.lstat(fpath)
               except:
                  pass

               output.append(self.lsone(fname, s))
                
         else:
            output.append(self.lsone(aname, s))
            
      sys.stdout.write(cPickle.dumps(output))
      return

   def sched(self, args=[]):
      """Start a scheduler for this node."""

      sl = SchedLock(self.Locations.this.rootdir)
      s = JobScheduler(self.Locations.this)

      f = file(os.devnull)
      os.dup2(f.fileno(), 0)

      # daemonify
      if not os.fork():
         # Middle-child
         os.setsid()
         if not os.fork():
            # Child

            f = file(self.Locations.this.rootdir + "/.schedlog-" + str(os.getuid()), 'a')
            os.dup2(f.fileno(), 1)
            os.dup2(f.fileno(), 2)

            # For now, the scheduler and FmJob still assume a global Locations var
            global Locations
            Locations = self.Locations
      
            if not sl.lock(): 
               # Somebody else running
               return 
      
            try:
               s.RunUntilDone()
            except:
               # Try very hard not leave a stale lockfile
               sl.unlock()
               raise
            sl.unlock()
            return
         else:
            self.Locations.cleanup = None #Don't cleanup in parent; child will

         exit(0) # Children should just exit now

      self.Locations.cleanup = None #Don't cleanup in parent; child will


   def wait(self, args):
      """Wait for a specified jobid to complete (and make sure scheduler is running)"""
      self.sched()

      jobfile = self.Locations.this.rootdir+"/jobs/"+args[0]
      #print >>sys.stderr, "Waiting on", jobfile

      sl = SchedLock(self.Locations.this.rootdir)

      delay = 0.1
      while os.path.isfile(jobfile):
         # Make sure scheduler is running
         if not sl.checkpid() and os.path.isfile(jobfile):
            print >>sys.stderr, "Error: scheduler exited while job still running.  Dumping log:"
            sys.stderr.writelines(file(self.Locations.this.rootdir + "/.schedlog-" + str(os.getuid())))
            return -1

         # Wait 
         # XXX: use famd
         time.sleep(delay)
         delay *= 2
         if delay > 1: delay = 1

def multiglob(globs):
   """Take a list of globs and return a list of all of the files that match any of those globs."""

   assert(type(globs) == type([]))
   ret = []
   for g in globs:
      ret += glob.glob(g)
   return ret

class SubprocessLs:
   def __init__(self, procs): 
      """Take a list of subprocesses"""
      self.procs = procs
      self.nextline = [None] * len(self.procs)

   def __iter__(self): 
      return self

   def next(self):
      smallest = None
      for i in self.procs:
           # Job slot is set to None after that job terminates
           if not self.procs[i]: continue

           # Get next line from this job if we don't already have one cached
           if not self.nextline[i]:
              self.nextline[i] = self.procs[i].stdout.readline()

              # Check to see if the job terminated
              #if not self.nextline[i]:
                 #print >>sys.stderr, i, "exited", self.procs[i].poll()

           # See if this job's next line should go before any others
           if self.nextline[i] and (smallest == None or self.nextline[i] < self.nextline[smallest]):
              smallest = i

      if smallest == None:
            #print >>sys.stderr, "Done; left", self.nextline
            raise StopIteration

      s = self.nextline[smallest]
      self.nextline[smallest] = None
      return s 

FmCommands(sys.argv[1:])
