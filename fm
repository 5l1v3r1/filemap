#!/usr/bin/python 
#
# Public Domain License:
#
# This program was prepared by Los Alamos National Security, LLC at
# Los Alamos National Laboratory (LANL) under contract
# No. DE-AC52-06NA25396 with the U.S. Department of Energy (DOE). All
# rights in the program are reserved by the DOE and Los Alamos
# National Security, LLC.  Permission is granted to the public to copy
# and use this software without charge, provided that this Notice and
# any statement of authorship are reproduced on all copies.  Neither
# the U.S. Government nor LANS makes any warranty, express or implied,
# or assumes any liability or responsibility for the use of this
# software.
#
# Author: Mike Fisk <mfisk@lanl.gov>
#

import ConfigParser, sys, optparse, random, urlparse, subprocess, os, glob, errno, string, socket, time, traceback, sha, pydoc, tempfile, re, socket, cStringIO, base64, stat, cPickle, pwd, grp, shlex

Verbose = 0

nonalphanumre = re.compile('\W')

def escape(s):
   (s, num) = nonalphanumre.subn(lambda m: "=%02X" % ord(m.group()), s)
   return s

def rm_rf(path):
   for (dirpath,dirs,files) in os.walk(path, topdown=False):
      for f in files: 
         os.unlink(dirpath + "/" + f)

      os.rmdir(dirpath)

def mkdirexist(path):
   try:
      os.makedirs(path)
   except OSError, e:
      if e.errno == errno.EEXIST:
         pass
      else:
         raise e

def rsyncDeWildcard(pth):
   """Take a path, potentially including wildcards, and reduce to rsync args that can't expand."""
   pth = pth.replace('//', '/').replace('/./','/')

   includes = [pth]

   # Reduce pth to a non-wildcard path
   while '*' in pth or '?' in pth:
      pth = os.path.dirname(pth)
      includes.append(pth + "/")

   # Include all parents of the path
   x = pth
   while True:
      dir = os.path.dirname(x)
      if dir == x: break
      x = dir
      includes.append(x + "/")

   return (includes, [pth])

def rsyncSimplify(lst):
   includes = []
   paths = []
   for l in lst:
         [newi, newp] = rsyncDeWildcard(l)
         includes += newi
         paths += newp
      
   includes = ["--include=" + x for x in includes] 
   includes.append("--exclude=*")
   return [includes, paths]
  
      
class MethodOptionParser(optparse.OptionParser):
   """OptionParser to be instantiated by dispatch methods of a class.  Sets %prog to be the calling function name and epilog to be the doc string for that function.  Also prints options in usage string more like man conventions instead of just [options]."""

   def __init__(self, *args, **kwargs):
      if not kwargs.has_key('epilog'):
         caller = sys._getframe(1)
         pmethod = caller.f_code.co_name
         pself = caller.f_locals['self']
         method = pself.__class__.__dict__[pmethod]
         kwargs['prog'] = pmethod
         self.Epilog = method.__doc__

      if kwargs.has_key('fixed'):
         self.fixed = kwargs['fixed']
         del kwargs['fixed']
      else:
         self.fixed = None

      optparse.OptionParser.__init__(self, *args, **kwargs)

   def get_usage(self):
      if self.fixed != None:
         flags = []
         options = ''
         for o in self.option_list:
            if o.action == 'store_true':
               flags += o._short_opts[0].lstrip('-')
            elif o.action == 'help':
               pass
            else:
               options += "[%s %s]" % (o._short_opts[0], o._long_opts[0].lstrip('-').upper())
               if o.action == 'append':
                  options += '...'
               
               options += ' '

         flags = string.join(flags, '')
         if flags: options += "[-" + flags + "] "
         
         self.usage = "%prog " + options + self.fixed

      return optparse.OptionParser.get_usage(self)

   def print_help(self):
      r = optparse.OptionParser.print_help(self)
      
      # For python < 2.5, we have to print our own epilog 
      print
      print self.Epilog
      return r

class FmLocations:
   def __init__(self, config=None, locs=[], stdin=False):
      self.locs = locs # Allow pick() to derive an instance with a subset of the classes

      if config:
         self.config = config

      else:
         self.config = ConfigParser.SafeConfigParser()

         if stdin:
            # Read config file from stdin
            self.config.readfp(sys.stdin)

            # Keep a copy of the config around so our children can reference it
            fd, name = tempfile.mkstemp()
            os.environ['FMCONFIG'] = name
            fh = os.fdopen(fd, 'w')
            self.config.write(fh)
            fh.close()

         else:
            filename = os.environ.get('FMCONFIG')
            #print >>sys.stderr, "Env filename is", filename
            if not filename:
               if os.path.isfile("filemap.conf"):
                  filename = "filemap.conf"
               else:
                  filename = "/etc/filemap.conf"

            # Read config file from stdin
            r = self.config.read(filename)
            if not r:
               raise Exception("Unable to locate config file.  Set FMCONFIG or place filemap.conf in . or /etc")

            #print >>sys.stderr, "Reading config", filename, self.config.sections()
            os.environ['FMCONFIG'] = filename

         self.replication = int(self.config_get("global", "replication", "1"))
         self.umask = int(self.config_get("global", "umask", "0002"))
         os.umask(self.umask)

      # Unless we were passed an explicit list of locations, grab all
      # from the config file.
      if not locs:
         for s in self.config.sections():
            if s == "global": continue
            l = FmLocation()
            l.name = s
            l.rootdir = os.path.expanduser(self.config_get(s, "rootdir"))
            l.jobdir = l.rootdir + "/jobs"
            l.hostname = self.config_get(s, "hostname")
            l.syncdir = os.path.expanduser(self.config_get(s, "syncdir"))

            pythoncmd = os.path.expanduser(self.config_get(s, "python", "python"))
            l.fmcmd = [pythoncmd, os.path.expanduser(self.config_get(s, "fm", l.rootdir + "/sys/fm"))]

            l.sshcmdstr = os.path.expanduser(self.config_get(s, "ssh", "ssh -o 'GSSAPIDelegateCredentials yes' -A", raw=True))
            l.sshcmd = shlex.split(l.sshcmdstr, comments=True)
            l.rsynccmd = shlex.split(self.config_get(s, "rsync", "rsync", raw=True), comments=True)
            l.rsynccmd[0] = os.path.expanduser(l.rsynccmd[0])

            self.locs.append(l)

      # Nail open SSH connections for low-latency muxing
      try:
         for l in self.locs:
            if l.hostname:
               #print "Setting-up persistent SSH connection to", l.hostname
               #subprocess.check_call(l.sshcmd + ["-o", "ControlMaster auto", "-fN", l.hostname], stdin=None)
               pass
      except:
         traceback.print_exc()
         print >>sys.stderr, """Warning, unable to use "ssh -o 'ControlMaster auto' -fN"; Will do without."""

   def config_get(self, section, key, defval=None, raw=False):
      if self.config.has_option(section, key):
         return self.config.get(section, key, raw=raw)
      elif self.config.has_option("global", key):
         return self.config.get("global", key, raw=raw)
      else:
         return defval

   def thisis(self, n):
      # This sets of an easy to use member to get the info for this Location
      self.this = self.locs[n]
      self.thisnum = n

   def pickn(self, seed=None):
      """Pick some pseudo-random locations.  The number of locations
      chosen is based on the replication factor in the config.
      Specify a seed to get deterministic results (e.g. for a given
      extension number.)  The return value is the list of indices into
      the current locations."""

      #print "pick seed=", seed
      candidates = range(0, len(self.locs))
      used = []
      if seed: random.seed(seed)

      for i in range(0, self.replication):
         if not candidates:
            print >>sys.stderr, "Cannot pick %d locations from %d available" % (n, len(self.locs))
            break
         l = random.choice(candidates)
         used.append(l)
         candidates.remove(l)
     
      #print >>sys.stderr, "pick", seed, "->", used

      return used
         
   def pick(self, seed=None, selfIsNoop=False):
      """Return a new FmLocations instance that uses a subset of the
locations of this instance."""

      used = self.pickn(seed)
      if selfIsNoop and self.thisnum in used: 
         used.remove(self.thisnum)
      return FmLocations(self.config, [self.locs[i] for i in used])
         
   def forAllLocal(self, cmd, stdout=None):
      """Same ase forAll(), but CMD will be run as arguments to "fm _local"."""

      cmd = ["fm", "_local", "-n", "%(NODENUM)"] + cmd
      fp = cStringIO.StringIO()
      self.config.write(fp)

      return self.forAll(cmd, subst=True, stdout=stdout, stdinstr=fp.getvalue())

   def forAll(self, Cmdv, src=[], dst=[], subst=False, trailer=None, glob=True, stdinstr=None, stdout=None, stderr=None, call=False, once=False):
      if src and type(src) != type([]): src = [src]
      if dst and type(dst) != type([]): dst = [dst]

      if stdinstr:
         std=subprocess.PIPE

      procs = Procs(stdinstr=stdinstr, stdout=stdout, stderr=stderr)

      nodenum = 0
      for loc in self.locs:
         if subst: 
            cmd = [n.replace("%(ROOTDIR)", loc.rootdir).replace("%(SYNCDIR)", loc.syncdir).replace("%(NODENUM)", str(nodenum)) for n in Cmdv]
         else:
            cmd = list(Cmdv)

         s = [loc.rootdir + "/" + f for f in src]
         d = [loc.rootdir + "/" + f for f in dst]

         if cmd[0] == "fm":
            cmd = loc.fmcmd + cmd[1:]
         if loc.hostname:
            cmd = loc.sshcmd + [loc.hostname] + cmd + s + d

         else:
            if s and glob:
               s = multiglob(s)
               if not s: continue

            if d and glob:
               d = multiglob(d)
               if not d: continue

            cmd += s + d 
            
         if trailer:
            cmd += trailer

         procs.Popen(cmd, call=call)

         nodenum += 1

         # Stop now if "once" argument was set
         if once and nodenum: return procs

      return procs
   
   def put(self, src, dst, procs=None):
      s = multiglob([src])

      if not s:
         print >>sys.stderr, "No such file:", src
         return procs

      if not procs: procs = Procs()

      for loc in self.locs:
         d = loc.rootdir + "/" + dst

         if loc.hostname:
            d = loc.hostname + ":" + d

         cmd = loc.rsynccmd + ["-a", "-e", loc.sshcmdstr] + s + [d]

         procs.Popen(cmd)

      return procs

   def get(self, args, procs=None, outfile=sys.stdout, relative=True):
      """Copy files from the cloud to local storage."""
      p = MethodOptionParser(fixed="src... dst")
      p.disable_interspersed_args()
      p.add_option("-c", "--cat", action="store_true", help="Cat files to stdout")
      p.add_option("-n", "--name", action="store_true", help="Show file name when catting files")
      (options, args) = p.parse_args(args)

      rsyncflags = "-rptgo"
      if relative: rsyncflags += "R"

      if options.cat:
         assert(len(args) > 0)
         dst = tempfile.mkdtemp() + "/"
         src = args
      else:
         assert(len(args) > 1)
         dst = args[-1]
         src = args[:-1]

      if not procs: procs = Procs()

      for loc in self.locs:
         # To make relative paths shorter in rsync >= 2.6.7 , put "/./" 
         # in the path between the rootdir and the relative path
         s = [loc.rootdir + "/./" + f for f in src]

         # Cleanup the path to encourage rsync to get "/./" magic right
         s = [i.replace('//','/') for i in s]

         #print loc.rootdir, src, s

         # Take the specified paths/globs and turn them into a list of simple (non-glob) paths and --include/exclude options
         [includes, s] = rsyncSimplify(s)

         if loc.hostname:
            s = [loc.hostname + ":" + string.join(s)]
         else:
            s = multiglob(s)
            if not s:
               #print >>sys.stderr, "No such file:", src
               continue

         cmd = loc.rsynccmd + includes
         cmd += [rsyncflags, "--copy-unsafe-links", "-e", loc.sshcmdstr] + s + [dst]

         procs.Popen(cmd)

      procs.collect(ignoreErrors=True)

      if options.cat:
         empty = True
         for (dirpath, dirnames, filenames) in os.walk(dst):
            for f in filenames:
               fname = os.path.join(dirpath, f)
               if os.path.getsize(fname):
                  if options.name:
                     relname = fname[len(dst):]
                     print >>outfile, "=== %s ===" % relname
                     empty = False
                  outfile.writelines(file(fname))
         if options.name and not empty:
            print >>outfile, "======"

         rm_rf(dst)

      return procs
         
class FmLocation:
   def __init__(self):
      pass

class ErrorFile:
   """This class implements a few file-like methods so that it can be used
   in place of a real file in some cases.  Whatever is written to this file
   is printed on stderr with a static prefix (like syslog)."""

   def __init__(self, prefix):
      self.prefix = prefix

   def write(self, buf):
      print >> sys.stderr, prefix, ":", buf

   def fileno(self): return sys.stderr.fileno()

class Procs:
   """This class is used to launch some number of child processes and reap them.
   It provides methods to instantiate a process for each node."""

   def __init__(self, stdinstr=None, stdout=None, stderr=None, stdin=None):
      self.stdout = stdout
      self.stderr = stderr
      self.stdin = stdin
      self.stdinstr = stdinstr
      self.procs = []

      if stdinstr:
         self.stdin = subprocess.PIPE

   def Popen(self, lst, prefix=None, call=False, tag=None):
      if not tag: tag = string.join(lst)

      if prefix == None: prefix = len(self.procs)
      if self.stderr:
         err = self.stderr
      else:
         err = ErrorFile(prefix)

      if Verbose > 2: print >>sys.stderr, lst
      r = subprocess.Popen(lst, stdin=self.stdin, stdout=self.stdout, stderr=err)
      r.tag = tag
      r.start = time.time()
      self.procs.append(r)

      if self.stdinstr:
         r.stdin.write(self.stdinstr)
         r.stdin.close()

   def collect(self, ignoreErrors=False):
      total = len(self.procs)
      errors = 0
      #print >>sys.stderr, "Need to collect", total, [x.pid for x in self.procs]
      while total:
         pid, status = os.waitpid(-1, 0)
         total += -1
         if status: errors += 1 
         if Verbose > 1:
            found = False
            for p in self.procs:
               if p.pid == pid:
                  print >>sys.stderr, "Process %s returned %d in %gs" % (p.tag, status, time.time() - p.start)
                  found = True
                  break
            if not found: print >>sys.stderr, "Unknown child %d returned" % (pid)
      self.procs = []
         
      if not ignoreErrors and errors:
         print >>sys.stderr, "Error, %d/%d subprocess(es) returned error" % (errors, total)
         sys.exit(-1)

      return (errors == 0)

# The design is as follows:
#   1. Jobs are submitted in an uncoordinated manner, so they have unique
#      identifiers that aren't generated by the user. but that are identical
#      across all nodes running a job. 
#   2. To manage the problem of listing jobs and removing jobs, we use a 
#      directory structure to contain all current jobs.  Deleting a file should
#      (eventually) lead to a job not running further.
#   3. New jobs should be invoked synchrously by calling a job scheduler with a
#      hint that points to the job.  Failure to send this hint should only delay
#      the job scheduler discovering the job since it should periodically poll
#      for changes in the job directory.
#   4. We assume out of band replication of the job dir across nodes (via NFS, 
#      rsync, etc.)

class JobScheduler:
   """JobScheduler maintains a set of child processes working on a set of jobs."""

   def __init__(self, options, numthreads=None):
      self.jobs = {}  # A dictionary of job objects indexed by job name
      self.procs = {} # A dictionary of subprocess.Popen objects index by pid
      self.options = options

      self.freethreads = numthreads 
      if not self.freethreads:
         self.freethreads = 4 #XXX: Use configfile setting

   def ReadJobDir(self):
      """Check for new/removed jobs"""

      jobs = os.listdir(self.options.jobdir)
      removed = set(self.jobs) - set(jobs)
      recent = set(jobs) - set(self.jobs)
      for j in removed:
         #print "Job", j, "removed"
         del self.jobs[j]

      for j in recent:
         #print "Job", j, "added"
         try:
            self.jobs[j] = FmJob(self.options.jobdir + "/" + j)
         except:
            traceback.print_exc()
            print >>sys.stderr, "Error parsing job description", j, "; skipping"

   def RunOnce(self):
         """Try to launch a work item and return True on success (None->nothing to do)"""

         self.freethreads -= 1

         # Look for something with work to do
         for job in self.jobs.keys(): #XXX, Should randomize order
            proc = self.jobs[job].compute(self.options)
            if proc: 
               #print >>sys.stderr, "Launched", proc.pid
               self.procs[proc.pid] = proc
               return True
            else:
               # No more work to do, check to see if job is complete
               # Note, job cannot be complete if parent is still active
               j = self.jobs[job]
               if not j.continuous and not j.running and (not j.parent or not self.jobs.has_key(j.parent)):
                  self.JobDone(job)

         self.freethreads += 1
         return None

   def JobDone(self, job):
      """A job has run to completion, remove it locally."""

      del self.jobs[job]
      #print >>sys.stderr, "Job", job, "done"
      os.unlink(self.options.jobdir + "/" + job)

   def RunUntilDone(self):
      """Run until there is no more input on it.  Return True iff we did something"""

      didSomething = False
      sleep = 0.0001

      while True:
         if self.freethreads:
            self.ReadJobDir()  
            if not len(self.procs) and not len(self.jobs):
               # Doing nothing and no more jobs to run
               return didSomething
            if self.RunOnce():
               didSomething = True
            else:
               # Nothing to do right now
               # Don't return because we're still working,
               # but wait before we look for more work to do.
               #print >>sys.stderr, "Wait for completion (and/or more work)"

               # Sleeping hurts latency for detecting new inputs, but avoids busy waits.
               # So we do an exponential backoff in how long we sleep with a max of 1 second
               time.sleep(sleep)
               sleep *= 10
               if sleep > 1: sleep = 1  # Max out at 1 sec sleep
         self.CollectChildren() # Will block

   def CollectChildren(self): 
      # This is a loop in case multiple children are exited
      while len(self.procs):
         if not self.freethreads:
            # Maxed out on threads, so wait for a child to finish
            #print >>sys.stderr, "Wait for a child to exit"
            opts = 0
         else:
            # Check for children, but don't block waiting
            opts = os.WNOHANG

         before = os.times()
         pid = None

         try:
            (pid, status) = os.waitpid(-1, opts) 
         except OSError, e:
            if e.errno == errno.ECHILD:
               # If we're, here its probably because in python 2.4, subprocess reaps other children
               # whenever making a new one.  So one of the children we thought was running had already
               # been waitpidded.  http://bugs.python.org/issue1731717
               # So, poll on each of our children to find the one to finalize:
               for p in self.procs:
                  status = p.poll()
                  if status: 
                     pid = p.pid
                     break      
            else:
               raise e

         if pid:
            after = os.times()
            #print "Process", pid, "exited with status", status
            self.finalize(self.procs[pid], status, after[2]-before[2], after[3]-before[3])
            del self.procs[pid]
            self.freethreads += 1
         else:
            # No children pending, so return
            return

   def finalize(self, p, status, utime, stime):
      #print >>sys.stderr, "Job completed", status
      print >>p.syncfile, status, utime, stime
      p.syncfile = None

      f = file(p.outdirname + "/." + p.outbasename + "/status", "w")

      print >>f, socket.gethostname(), os.getpid()
      print >>f, status, utime, stime
      f.close()
   
      dst = p.outdirname + "/" + p.outbasename 
      if os.path.exists(dst): 
          rm_rf(dst)
          # By removing this dst, we're invalidating any previously derived data, but our redo-if-newer-than logic will handle the compute.

      os.rename(p.outdirname + "/." + p.outbasename, dst)
      p.job.running -= 1 

      del p
     
class FmJob:
   """Each FmJob object represents a job, as specified in a job file, and
    provides methods for identifying and processing inputs for that job."""

   def __init__(self, fname):
      config = ConfigParser.SafeConfigParser()

      processed = config.read(fname)
      if not processed:  
         raise IOError(errno.ENOENT, "Could not open file", fname)

      self.jobname = os.path.basename(fname)
      self.cmd = config.get("mrjob", "cmd", raw=True)
      self.inputs = config.get("mrjob", "inputs")
      self.continuous = config.has_option("mrjob", "continuous")
      self.reduce = config.has_option("mrjob", "reduce")
      if config.has_option("mrjob", "parent"):
         self.parent = config.get("mrjob", "parent")
      else:
         self.parent = None

      # Number of threads currently processing data for this job.
      self.running = 0

   def compute(self, options):
      inputs = multiglob([options.rootdir + "/" + x for x in self.inputs.split()])

      if self.reduce:
         exts = [os.path.basename(i) for i in inputs]
         exts = list(set(exts))  # Get unique extensions

         for ext in exts:
            # Don't process .d directories as named inputs (feedback loop)
            if ext[-2:] == ".d": continue

            # Apply all of the files with the same extension,
            # but only if that extension belongs on this node.
            choices = Locations.pickn(ext)
            if Locations.thisnum not in choices:
               #print >>sys.stderr, Locations.thisnum, "hashes", ext, "to", choices
               continue

            #Get list of files with this extension
            xlen = len(ext)
            files = []
            for i in inputs:
               if i[-xlen:] == ext:
                  files.append(i)

            outfilename = "/reduce/" + self.jobname + "." + ext

            p = self.computeItem(files, outfilename, options)
            if p: 
               #print >>sys.stderr, "Reduce ext", ext, "on node", Locations.thisnum, files, "of", self.inputs

               return p

         return None # Nothing to do

      for i in inputs:
         if i[-2:] == ".d": 
            # Don't process .d directories as named inputs (feedback loop)
            continue

         bname = os.path.basename(i)

         relativename = i[len(options.rootdir):]
         outfilename = relativename + ".d/" + escape(self.cmd)
         
         p = self.computeItem(i, outfilename, options)
         if p: return p

      return None # Nothing to do

   def computeItem(self, inputs, outfilename, options):
      """Start (but don't wait for completion) running this job on of the next unprocessed input for this job."""

      if type(inputs) != type([]):
         inputs = [inputs]

      syncfilename = options.syncdir + "/" + outfilename + "/status"
      mkdirexist(os.path.dirname(syncfilename))
      try:
         syncfile = os.fdopen(os.open(syncfilename, os.O_CREAT|os.O_SYNC|os.O_EXCL|os.O_WRONLY), "w")
      except OSError, e:
         if e.errno == errno.EEXIST:
            # This is a common case; somebody has already grabbed
            # the synchronization file, so we don't need to process
            # this file --- unless in the inputs are newer than the outputs
            lasttime = os.path.getmtime(syncfilename)
            redo = False
            for i in inputs:
               if os.path.getmtime(i) > lasttime:
                  redo = True
                  break

            if not redo:
               return None
               
            # Nuke the syncfile
            try:
               os.unlink(syncfilename)
               syncfile = os.fdopen(os.open(syncfilename, os.O_CREAT|os.O_EXCL|os.O_WRONLY), "w")
            except:
               # Somebody else was doing this at the same time, let them have it
               return None
            
         else:
            raise e

      # Now we hold the "lock" on the syncfile, so proceed...

      print >>syncfile, socket.gethostname(), os.getpid()
      syncfile.flush()

      obase = options.rootdir + "/" + outfilename

      # Now insert a "." at the beginning of the basename for partial output
      odirname = os.path.dirname(obase)
      obasename = os.path.basename(obase)
      obase = odirname + "/." + obasename

      oname = obase + "/0"
      ename = obase + "/stderr"
      mkdirexist(obase)
      sout = os.fdopen(os.open(oname, os.O_CREAT|os.O_WRONLY), "w")
      serr = os.fdopen(os.open(ename, os.O_CREAT|os.O_WRONLY), "w")

      cmd = self.cmd

      if '%(input)' in cmd:
         cmd = cmd.split()
         iidx = cmd.index('%(input)')
         cmd[iidx:iidx] = inputs
      else:
         cmd = cmd.split() + inputs
    
      if cmd[0] == "fm":
         cmd = options.fmcmd + cmd[1:]

      for c in cmd:
         cmd = [os.path.expanduser(c) for c in cmd]
 
      try:
         p = subprocess.Popen(cmd, stdout=sout, stderr=serr, cwd=obase)
      except:
         print >>sys.stderr, "Error", sys.exc_value, "executing", cmd
         return None

      p.syncfile = syncfile
      p.outdirname = odirname
      p.outbasename = obasename
      p.job = self
      self.running += 1
      sout.close()
      serr.close()

      #print >>sys.stderr, "computeItem", cmd, socket.gethostname(), p.pid
      return p

class CommandSetBase(object):
   """This is a base-class for defining methods which are used as
   command line-based commands.  You should inherit from it and define
   methods.  All methods will be callable.  Usage and help information
   will only include methods that do not begin with an underscore."""

   def __method(self, mname):
      return self.__class__.__dict__[mname]

   def __usage(self):
      """Return a multi-line usage string based on the defined methods and
their doc strings."""

      usage = 'For more help, specify one of the following commands followed by -h option:\n'
      keys = self.__class__.__dict__.keys()
      keys.sort()
      for cmd in keys:
         if cmd[0] == "_": continue
         method = self.__method(cmd)
         #docstr = pydoc.text.indent(pydoc.text.document(method)).replace("(self, args)","")
         docstr = method.__doc__
         if docstr:
            docstr = docstr.split("\n")[0].strip().rstrip(".")
         usage += "   %-8s - %s\n" % (cmd, docstr)
      return usage

   def __init__(self, args):
      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.set_usage("""%prog command args...\n""" + self.__usage())
      (options, args) = p.parse_args(args)
   
      if not args: 
         p.print_help()
         sys.exit(1)

      try:
         method = self.__method(args[0])

      except:
         print >>sys.stderr, "Unknown command ", args[0]
         p.print_help()
         sys.exit(1)

      method(self, args[1:])

class FmCommands(CommandSetBase):
   def __init__(self, *args):
      self._locs = None
      CommandSetBase.__init__(self, *args)

   def _Locations(self):
      if not self._locs: 
         self._locs = FmLocations()
      return self._locs

   def split(self, args):
      """Partition an input file.
Split an inputfile into n pieces.  By default, the first
whitespace-delimited field is used as the key.  All lines with the
same key will be placed in the same output file.  The -r option can be
used to specify a Perl-compatible regular expression that matches the
key.  If the regex contains a group, then what matches the group is
the key; otherwise, the whole match is the key.

The root of the output file names must be specified either on the
command line or in the FMOUTPUT environment variable (which is set for
all programs running as FM jobs).  
"""
      p = MethodOptionParser(fixed="infile")
      p.add_option("-n", "--nways", help="Number of output files to use")
      p.add_option("-r", "--regex", help="Regex that matches the key portion of input lines")
      (options, args) = p.parse_args(args)
      options.nways = int(options.nways)

      assert(len(args))
      infile = args[0]

      #print >>sys.stderr, "Writing to", ofile

      if not options.nways:
         print >>sys.stderr, "-n option required"
         return False
      
      if not options.regex:
         options.regex = '^([^\s]*)'
         
      options.regex = re.compile(options.regex)

      files = []
      for i in range(0, options.nways):
         fname = str(i+1)
         files.append(file(fname, "w"))

      for line in file(infile):
         key = options.regex.search(line)
         if key:
            g = key.groups()
            if len(g):
               key = g[0]
            else:
               key = key.group(0)
         else:
            print >>sys.stderr, "Key not found in line:", line.rstrip()

         i = hash(key) % options.nways
         files[i].write(line)

      for f in files: f.close()

   def kill(self, args):
      """Kill a job."""
      (options, args) = MethodOptionParser(fixed="jobid").parse_args(args)
      if len(args) < 1:
         print >>sys.stderr, "Must specify a JobId to kill"
         return False
      args = ["/jobs/" + a for a in args]
      self._Locations().forAll(["rm", "-f"], args).collect()

   def mv(self, args):
      """Rename files in the cloud"""
      (options, args) = MethodOptionParser(fixed="src... dst").parse_args(args)
      if len(args) < 2:
         print >>sys.stderr, "mv requires at least 2 arguments"
         return False

      dst = args[-1]
      each = args[:-1]
      if len(each) > 1: dst += "/"
 
      return self._Locations().forAll(["mv"], each, dst).collect()

   def mkdir(self, args, absolutes=[]):
      """Make a directory (or directories) on all nodes.  
Has unix "mkdir -p" semantics."""
      (options, args) = MethodOptionParser(fixed="dir...").parse_args(args)
      # Make sure destination exists
      self._Locations().forAll(["mkdir", "-p", "-m", "2775"] + absolutes, args, subst=True, glob=False).collect()

   def jobs(self, args):
      """Show all of the jobs still active."""
      (options, args) = MethodOptionParser(fixed="").parse_args(args)

      tmpdir = tempfile.mkdtemp()

      self._Locations().get(["/jobs/*", tmpdir], relative=False)
      fmt = "%-28s %-15s %s"
      print fmt % ("Job ID","Command", "Inputs")
        
      for f in os.listdir(tmpdir):
         j = FmJob(os.path.join(tmpdir, f))
         print fmt % (f, j.cmd, j.inputs)

      rm_rf(tmpdir)
  
   def df(self, args):
      """Show free space on each node."""
      (options, args) = MethodOptionParser(fixed="").parse_args(args)
      self._Locations().forAll(["df", "-h"], "/", glob=False).collect()

   def chmod(self, args, absolutes=[]):
      """Change permissions on files in the cloud."""
      (options, args) = MethodOptionParser(fixed="mode files...").parse_args(args)
      self._Locations().forAll(["chmod", args[0]] + absolutes, args[1:], subst=True).collect()

   def chgrp(self, args, absolutes=[]):
      """Change GID on files in the cloud."""
      (options, args) = MethodOptionParser(fixed="perm files...").parse_args(args)
      self._Locations().forAll(["chgrp", args[0]] + absolutes, args[1:], subst=True).collect()

   def store(self, args):
      """Store one or more files into the cloud."""

      """src... dst

Copy the specified file(s) into the virtual store.  
"""
      (options, args) = MethodOptionParser(fixed="files... dst").parse_args(args)

      assert(len(args) > 1)
      dst = args[-1]
      args = args[:-1]
      if len(args) > 1: dst += "/"

      procs = Procs()
      # Iterate over args in batches.
      # Since hashing may not be even and we want each node to be doing something,
      # we use 1.5 * NODES / replication as the batch size
      while args:
         for i in range(0, int(1.5 * len(self._Locations().locs) / self._Locations().replication)):
            if not args: break
            a = args.pop(0)
            self._Locations().pick().put(a, dst+"/", procs=procs)
         procs.collect()
         

   def map(self, args):
      """Launch a computation on a set of input files in the cloud.
Run the specified command on each input file (in the virtual store)
described by the inputglob.  Multiple inputglob arguments can be
given.  The -c option says that the commond should continue to run on
new inputs as they arrive.

Multiple commands can be chained together with |.  Each output file of
the first command becomes an input for the next command in the
pipeline.

The -f option says that any previously cached output should be ignored
and the program re-run.
"""

      p = MethodOptionParser(fixed="cmd [| cmd...]")
      p.disable_interspersed_args()
      p.add_option("-i", "--inputglob", action="append", help="Glob of input files (in root)")
      p.add_option("-c", "--continuous", action="store_true", help="Continue to look for new input files to arrive")
      p.add_option("-f", "--fresh", action="store_true", help="Do not use any cached output")
      (options, cmd) = p.parse_args(args)
      
      cmds = string.join(cmd).split("|")
      inglobs = options.inputglob
      outglobs = []
      assert (type(inglobs) == type([]))

      jobname = None
      for cmd in cmds:
         reduce = False
         cmd = cmd.strip()
         if cmd[0] == ">":
            cmd = cmd.lstrip(">")

            # First, re-distribute the data
            restore = "fm _local restore -s" 
            jobname = self._MapComponent(restore, inglobs, options, parent=jobname)
            outglobs += [i + ".d/" + escape(restore) for i in inglobs]
            # There is no change to the inglobs directory since restore puts files in the same virtual path they started in

            # Barrier; Wait for that job (and its parents) to finish:
            # But first, make sure a scheduler is running
            self._Locations().forAllLocal(["sched"]).collect()

            print >>sys.stderr, "Waiting for distribute to complete before reducing.  Job", jobname
            self._Locations().forAllLocal(["wait", jobname]).collect()

            reduce = True

         jobname = self._MapComponent(cmd, inglobs, options, reduce=reduce, parent=jobname)

         # Each component of pipeline uses previous output as input
         if reduce:
            outglobs += ["/reduce/" + jobname + ".[0-9]*/[0-9]*"]
            inglobs = ["/reduce/" + jobname + ".[0-9]*/[0-9]*"]
         else:
            outglobs += [i + ".d/" + escape(cmd) for i in inglobs]
            inglobs = [i + ".d/" + escape(cmd) + "/[0-9]*" for i in inglobs]

      self._Locations().forAllLocal(["sched"]).collect()

      print >>sys.stderr, "Waiting for completion.  Job", jobname
      self._Locations().forAllLocal(["wait", jobname]).collect()

      # Print any stderr files to stderr
      errglobs = [f + "/stderr" for f in outglobs]
      self.get(["-cn"] + errglobs, outfile=sys.stderr)

      # Get the status files
      statglobs = [f + "/status" for f in outglobs]
      loc = tempfile.mkdtemp()
      self.get(statglobs + [loc])

      # Tally up info from status files
      utime = stime = 0.0
      errcodes = {}
      for (dirpath, dirnames, filenames) in os.walk(loc):
         for f in filenames:
            fname = os.path.join(dirpath, f)
            #print >>sys.stderr, "status", fname
            f = file(fname)
            line = f.readlines()[1]
            stats = line.split()
            ec = os.WEXITSTATUS(int(stats[0]))
            errcodes[ec] = errcodes.get(ec, 0) + 1
            utime += float(stats[1])
            stime += float(stats[2])
      rm_rf(loc)

      # Print out summary info
      errstr = ''
      for k in errcodes.keys():
         if not errstr: 
            errstr = "%d processes returned %d" % (errcodes[k], k)
         else:
            errstr += "; %d x %d" % (errcodes[k], k)
      if not errstr: errstr = 'No processes ran'
      print >>sys.stderr, errstr, "in %gu+%gs CPU seconds." % (utime, stime)
  
      # Print the results to stdout
      self.get(["-c"] + inglobs)

   def get(self, *args, **kwargs):
      """Copy one or more files from the cloud ot local storage."""
      return self._Locations().get(*args, **kwargs)

   def cat(self, args):
      """Concatenate one or more files to stdout."""
      (options, args) = MethodOptionParser(fixed="files...").parse_args(args)

      procs = self._Locations().forAll(["cat"], args, stdout=subprocess.PIPE)
      for f in procs.procs:
            sys.stdout.writelines(f.stdout)
      return procs.collect(ignoreErrors=True)

   def _local(self, args):
      """This is for slave (remote) instantiations."""
      FmLocalCommands(args)

   def ls(self, args):
      """File & directory listing."""
      (options, args) = MethodOptionParser(fixed="files...").parse_args(args)

      if not args: args = ["/"]
      procs = self._Locations().forAllLocal(["ls"] + args, stdout=subprocess.PIPE)
      ls = {}
      for p in procs.procs:
         pkl = p.stdout.read()
         if not pkl: continue

         try:
            files = cPickle.loads(pkl)
         except:
            print >>sys.stderr, "Error", sys.exc_value, "parsing remote ls results:", pkl
            return -1

         for f in files:
            fname = f['name']
            if not ls.has_key(fname): ls[fname] = []
            ls[fname] += [f]

      keys = ls.keys()
      keys.sort()
      for k in keys:
         base = ls[k][0]
         nodes = [base['node']]
         isdir = base['perms'][0] == 'd'
         if isdir: base['size'] = 0
         criteria = ['perms', 'user', 'group', 'size']
         if not isdir: criteria.append('size')
         

         for i in ls[k][1:]:
            if isdir: i['size'] = 0

            # See if this node's instance matches the right criteria to be
            # collapsed into a single output line
            match = True
            for a in criteria:
               if base[a] != i[a]: 
                  # Does not match
                  match = False
                  break

            if match:
               # If you specify the same file twice, we don't want
               # the node number(s) duplicated in the node list
               if i['node'] not in nodes:
                  nodes.append(i['node'])
            else:
               # No match, so output a line for this instance
               print self._lsline(i)

         # Update the nodelist for this base instance and print it out
         base['node'] = nodes
         print self._lsline(base)
         
      return 

   def _lsline(self, attrs):
      size = "%-.2g" % attrs['size']
      size = size.replace("e+", "e").replace("e0", "e")

      nodes = attrs['node']
      if type(nodes) != type([]): nodes = [nodes]

      if len(nodes) > 1:
         attrs['mtime'] = '-'
      else:
         lt = time.localtime(attrs['mtime'])
         if lt[0] == time.localtime()[0]:
            attrs['mtime'] = time.strftime("%b %d %H:%M", lt)
         else:
            attrs['mtime'] = time.strftime("%b %d  %Y", lt)
               
      if len(nodes) == len(self._Locations().locs):
         nodes = '*'
      else:
         nodes = [str(i) for i in nodes]
         nodes = string.join(nodes,',')

      return "%s %3s %8s %8s %6s %12s %s" % (attrs['perms'], nodes, attrs['user'], attrs['group'], size, attrs['mtime'], attrs['name'])

   def rm(self, args):
      """Delete a file or directory tree in the cloud.
Remove empty file or directory.  Normal rm flags are passed through; -f is implied.
"""
      p = MethodOptionParser(fixed="files...")
      p.add_option('-R', action='store_true', help="recurse")
      (options, args) = p.parse_args(args)
      if options.R:
         options = ["-R"]
      else:
         options = []

      return self._Locations().forAll(["rm", "-f"] + options, args).collect()


   def init(self, args):
      """Create/correct directory structure.
 If a groupname is specified, it will be applied to the directories."""
      (options, args) = MethodOptionParser(fixed="[group]").parse_args(args)

      self.mkdir(["/", "/jobs", "/reduce", "/sys"], absolutes=["%(SYNCDIR)"])
      if args:
         self.chgrp([args[0], "/", "/jobs", "/reduce", "/sys"], absolutes=["%(SYNCDIR)"])
      self.chmod(["g+rwx,g+s", "/", "/jobs", "/reduce"], absolutes=["%(SYNCDIR)"])
      self._Locations().put(__file__, "/sys/fm").collect()
 
   def _MapComponent(self, cmd, inglobs, options, reduce=False, parent=None):
      jobdesc = "[mrjob]\ncmd = %s\ninputs = %s\n" % (cmd, string.join(inglobs))
      if parent: jobdesc += "parent = %s\n" % (parent)

      if options.continuous:
         jobdesc += "continuous = True\n"

      if reduce:
         jobdesc += "reduce = True\n"

      hash = sha.sha(jobdesc).digest()
      hash = base64.b64encode(hash, '+_')  # Normal b64 but / is _
      hash = hash.rstrip('\n\r =')
      jobfilename = "/tmp/" + hash
      jobfile = open(jobfilename, "w")
      jobfile.write(jobdesc)
      jobfile.close()

      if options.fresh:
         if reduce:
            syncfiles = ["%(SYNCDIR)/reduce/" + hash + "/[0-9]*"]
         else:
            syncfiles = ["%(SYNCDIR)/" + g + ".d/" + escape(cmd) + "/status" for g in inglobs] 

         # Delete sync files so that computation is forced to (re-)execute
         # These files are shared, so our forAll() is set to do only one node (once=True)
         self._Locations().forAll(["rm", "-f"] + syncfiles,  once=True, subst=True).collect()

         # Only force the first stage of the pipeline.  Out-of-date checks will handle downstream.
         options.fresh = False

      # Install the job on each node
      self._Locations().put(jobfilename, "/jobs/").collect()

      return hash

class FmLocalCommands(CommandSetBase):
   def __init__(self, args):
      p = optparse.OptionParser()
      p.disable_interspersed_args()
      p.add_option("-n", "--nodenum", help="Id number of this node")
      (options, cmd) = p.parse_args(args)

      if options.nodenum:
         # If -n is given, then we are a direct slave and 
         # should get our config on stdin
         os.environ['FMNODENUM'] = options.nodenum
         self.Locations = FmLocations(stdin=True)
      else:
         # We are a child of a scheduler and
         # should pick-up the config from an environment variable
         options.nodenum = os.environ['FMNODENUM']
         self.Locations = FmLocations()

      self.Locations.thisis(int(options.nodenum))

      try:
         CommandSetBase.__init__(self, cmd)
      except:
         traceback.print_exc()

   def restore(self, args):
      """[-s] files...

Iff -s is specified, then nodes should be chosen based on a pure
function of the file's suffix so that similarly split files will be
stored on the same nodes.  Also file will be stored into the same
directory in the virtual store that it originated in.
"""


      p = MethodOptionParser()
      p.disable_interspersed_args()
      p.add_option("-s", "--suffix", action="store_true", help="Store based on file's suffix")
      (options, args) = p.parse_args(args)
   
      procs = Procs()
      n = None
      for f in args:
         dst = os.path.dirname(f[len(self.Locations.this.rootdir):]) + "/"
         
         if options.suffix: 
            ext = os.path.basename(f)
            n = ext

         # Pick some destinations.  If the algorithm picks this node,
         # that is reasonable, but is a NOOP
         dsts = self.Locations.pick(n, selfIsNoop=True)
         #if options.suffix: print >>sys.stderr, "Extension", ext, "to", [(x.hostname or "") + ":" + x.rootdir + "/" + dst for x in dsts.locs]
         dsts.forAll(["mkdir", "-p"], dst, glob=False)
         dsts.put(f, dst, procs=procs)

      procs.collect()

   def lscharmod(self, perms, offset, chr):
      perms = list(perms)
      if perms[offset] == "-": 
           perms[offset] = chr.upper()
      else:
           perms[offset] = chr.lower()
      return string.join(perms, '')

   def lsone(self, fname, fs):
      # Convert to ls-style human-readable permission mask
      perms = "?"
      for kind in "BLK", "CHR", "DIR", "LNK", "SOCK", "FIFO", "REG":
         if getattr(stat, "S_IS"+kind)(fs.st_mode):
            perms = kind[0].lower().replace("f","p")
      if perms == "r": perms = "-"
         
      for who in "USR", "GRP", "OTH":
         for what in "R", "W", "X":
            if fs.st_mode & getattr(stat,"S_I"+what+who):
               c = what.lower()
            else:
               c = "-"
            perms += c

      if stat.S_ISUID & fs.st_mode: perms = self.lscharmod(perms, 3, "s")
      if stat.S_ISGID & fs.st_mode: perms = self.lscharmod(perms, 6, "s")
      if stat.S_ISVTX & fs.st_mode: perms = self.lscharmod(perms, 9, "t")
 
      # Construct the dict that is pickled and passed to the client
      d = {}
      d['name'] = fname
      d['node'] = self.Locations.thisnum
      d['perms'] = perms
      d['user'] = pwd.getpwuid(fs.st_uid).pw_name
      d['group'] = grp.getgrgid(fs.st_gid).gr_name
      d['size'] = fs.st_size
      d['mtime'] = fs.st_mtime
      return d

   def ls(self, args):
      os.chdir(self.Locations.this.rootdir)  
      #args = [(a.lstrip("/")).rstrip("/") for a in args] # Make relative; even if it started with a /
      procs = Procs()
      args = multiglob(["./" + a for a in args])
      if not args: return
      output = []
      for apath in args:

         # aname is the virtual name of the file; will be shown to user.
         # So strip the leading ./ that we added to make it relative
         aname = apath[2:].rstrip('/')

         s = os.stat(apath)
         if stat.S_ISDIR(s.st_mode):
            for f in os.listdir(apath):
               fpath = apath + "/" + f
               fname = aname + "/" + f
               output.append(self.lsone(fname, os.stat(fpath)))
         else:
            output.append(self.lsone(aname, s))
            
      sys.stdout.write(cPickle.dumps(output))
      return


   def sched(self, args):
      sockname = self.Locations.this.rootdir + "/.fmsock-" + str(os.getuid())
      s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)

      try:
         s.connect(sockname)
         ## Right now this is just for making sure a scheduler is running;
         ## We don't actually send anything over this socket (yet).
         #s.write(string.join(args) + "\n")
         #print >>sys.stderr, "Connected to scheduler"
         return

      except: 
         #print >>sys.stderr, "Warning", sys.exc_type, sys.exc_value, "; assuming scheduler not running yet"
         newname = sockname + "." + str(os.getpid())
         s.bind(newname)
         s.listen(1000) #XXX: We need to harvest these in the scheduler loop
         os.rename(newname, sockname)

      # If we get here, then we've bound as the (new) server

      # For now, the scheduler and FmJob still assume a global Locations var
      global Locations
      Locations = self.Locations

      s = JobScheduler(self.Locations.this)
      f = file(os.devnull)
      os.dup2(f.fileno(), 0)

      # daemonify
      if os.fork():
         return 

      os.setsid()
      if os.fork():
         return

      f = file(self.Locations.this.rootdir + "/.schedlog-" + str(os.getuid()), 'a')
      os.dup2(f.fileno(), 1)
      os.dup2(f.fileno(), 2)

      s.RunUntilDone()

   def wait(self, args):
      """Wait for a specified jobid to complete"""

      jobfile = self.Locations.this.rootdir+"/jobs/"+args[0]
      #print >>sys.stderr, "Waiting on", jobfile

      delay = 0.001
      while os.path.isfile(jobfile):
         time.sleep(1)
         if delay < 1: delay *= 2

def multiglob(globs):
   """Take a list of globs and return a list of all of the files that match any of those globs."""

   assert(type(globs) == type([]))
   ret = []
   for g in globs:
      ret += glob.glob(g)
   return ret

class SubprocessLs:
   def __init__(self, procs): 
      """Take a list of subprocesses"""
      self.procs = procs
      self.nextline = [None] * len(self.procs)

   def __iter__(self): 
      return self

   def next(self):
      smallest = None
      for i in range(0, len(self.procs)):
           # Job slot is set to None after that job terminates
           if not self.procs[i]: continue

           # Get next line from this job if we don't already have one cached
           if not self.nextline[i]:
              self.nextline[i] = self.procs[i].stdout.readline()

              # Check to see if the job terminated
              #if not self.nextline[i]:
                 #print >>sys.stderr, i, "exited", self.procs[i].poll()

           # See if this job's next line should go before any others
           if self.nextline[i] and (smallest == None or self.nextline[i] < self.nextline[smallest]):
              smallest = i

      if smallest == None:
            #print >>sys.stderr, "Done; left", self.nextline
            raise StopIteration

      s = self.nextline[smallest]
      self.nextline[smallest] = None
      return s 

FmCommands(sys.argv[1:])
